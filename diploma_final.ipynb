{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eR1y8kIBQVI"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall numpy gensim spacy adjustText umap-learn pymorphy3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaqJqKOC4BlK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from gensim.models import KeyedVectors\n",
        "import zipfile\n",
        "import gzip\n",
        "import shutil\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "from collections import defaultdict\n",
        "\n",
        "class RusVectoresAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.morph = MorphAnalyzer()\n",
        "        self.pos_cache = defaultdict(dict)\n",
        "\n",
        "    def _validate_model_file(self, file_path):\n",
        "        \"\"\"Проверка целостности файла модели\"\"\"\n",
        "        try:\n",
        "            print(f\"\\nПроверка файла модели: {file_path}\")\n",
        "            print(f\"Размер файла: {os.path.getsize(file_path) / (1024*1024):.2f} MB\")\n",
        "\n",
        "            if file_path.endswith('.gz'):\n",
        "                print(\"Проверка gzip архива...\")\n",
        "                with gzip.open(file_path, 'rb') as f:\n",
        "                    f.read(100)\n",
        "                print(\"Gzip архив валиден\")\n",
        "\n",
        "            elif file_path.endswith('.zip'):\n",
        "                print(\"Проверка zip архива...\")\n",
        "                with zipfile.ZipFile(file_path) as z:\n",
        "                    bad_file = z.testzip()\n",
        "                    if bad_file:\n",
        "                        raise ValueError(f\"Поврежденный файл в архиве: {bad_file}\")\n",
        "                print(\"Zip архив валиден\")\n",
        "\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка проверки файла: {e}\")\n",
        "            return False\n",
        "\n",
        "    def download_model(self):\n",
        "        \"\"\"Улучшенная загрузка модели с проверками\"\"\"\n",
        "        MODEL_URL = \"https://vectors.nlpl.eu/repository/20/220.zip\"\n",
        "        ZIP_PATH = \"rusvectores_model.zip\"\n",
        "        MODEL_FILE = None\n",
        "\n",
        "        # Шаг 1: Загрузка архива\n",
        "        if not os.path.exists(ZIP_PATH):\n",
        "            print(f\"\\nЗагрузка модели с {MODEL_URL}...\")\n",
        "            try:\n",
        "                response = requests.get(MODEL_URL, stream=True)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                total_size = int(response.headers.get('content-length', 0))\n",
        "                if total_size < 1024*1024:\n",
        "                    raise ValueError(f\"Слишком маленький файл ({total_size} bytes)\")\n",
        "\n",
        "                with open(ZIP_PATH, 'wb') as f, tqdm(\n",
        "                    desc=\"Прогресс\",\n",
        "                    total=total_size,\n",
        "                    unit='iB',\n",
        "                    unit_scale=True,\n",
        "                ) as bar:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        if chunk:\n",
        "                            f.write(chunk)\n",
        "                            bar.update(len(chunk))\n",
        "\n",
        "                if not self._validate_model_file(ZIP_PATH):\n",
        "                    raise ValueError(\"Загруженный архив поврежден\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка загрузки: {e}\")\n",
        "                if os.path.exists(ZIP_PATH):\n",
        "                    os.remove(ZIP_PATH)\n",
        "                raise\n",
        "\n",
        "        # Шаг 2: Распаковка архива\n",
        "        print(\"\\nПоиск файлов модели в архиве...\")\n",
        "        try:\n",
        "            with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "                file_list = zip_ref.namelist()\n",
        "                print(\"Содержимое архива:\")\n",
        "                for f in file_list:\n",
        "                    print(f\" - {f} (размер: {zip_ref.getinfo(f).file_size / (1024*1024):.2f} MB)\")\n",
        "\n",
        "                # Ищем файлы модели (поддерживаемые расширения)\n",
        "                valid_extensions = ['.bin', '.vec', '.txt', '.model', '.gz']\n",
        "                model_files = [f for f in file_list if any(f.endswith(ext) for ext in valid_extensions)]\n",
        "\n",
        "                if not model_files:\n",
        "                    raise FileNotFoundError(\"Не найдены файлы модели в архиве\")\n",
        "\n",
        "                # Выбираем первый подходящий файл\n",
        "                MODEL_FILE = model_files[0]\n",
        "                print(f\"\\nИзвлечение файла модели: {MODEL_FILE}\")\n",
        "                zip_ref.extract(MODEL_FILE)\n",
        "\n",
        "                # Проверка извлеченного файла\n",
        "                if not os.path.exists(MODEL_FILE):\n",
        "                    raise FileNotFoundError(f\"Файл {MODEL_FILE} не был извлечен\")\n",
        "\n",
        "                print(f\"Успешно извлечен: {MODEL_FILE} (размер: {os.path.getsize(MODEL_FILE) / (1024*1024):.2f} MB)\")\n",
        "\n",
        "                # Распаковка gzip если нужно\n",
        "                if MODEL_FILE.endswith('.gz'):\n",
        "                    uncompressed_file = MODEL_FILE[:-3]\n",
        "                    print(f\"\\nРаспаковка gzip: {MODEL_FILE} -> {uncompressed_file}\")\n",
        "\n",
        "                    with gzip.open(MODEL_FILE, 'rb') as f_in:\n",
        "                        with open(uncompressed_file, 'wb') as f_out:\n",
        "                            shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "                    if not os.path.exists(uncompressed_file):\n",
        "                        raise ValueError(\"Ошибка распаковки gzip\")\n",
        "\n",
        "                    MODEL_FILE = uncompressed_file\n",
        "                    print(f\"Успешно распакован: {MODEL_FILE} (размер: {os.path.getsize(MODEL_FILE) / (1024*1024):.2f} MB)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка распаковки: {e}\")\n",
        "            raise\n",
        "\n",
        "        # Шаг 3: Загрузка модели с проверкой\n",
        "        print(\"\\nПопытка загрузки модели...\")\n",
        "        try:\n",
        "            # Определяем формат файла\n",
        "            if MODEL_FILE.endswith('.bin'):\n",
        "                print(\"Загрузка в бинарном формате...\")\n",
        "                self.model = KeyedVectors.load_word2vec_format(MODEL_FILE, binary=True)\n",
        "            elif MODEL_FILE.endswith('.vec') or MODEL_FILE.endswith('.txt'):\n",
        "                print(\"Загрузка в текстовом формате...\")\n",
        "                self.model = KeyedVectors.load_word2vec_format(MODEL_FILE, binary=False)\n",
        "            else:\n",
        "                print(\"Попытка загрузки через gensim...\")\n",
        "                self.model = KeyedVectors.load(MODEL_FILE)\n",
        "\n",
        "            # Проверка загруженной модели\n",
        "            if not hasattr(self.model, 'vectors'):\n",
        "                raise AttributeError(\"Модель не содержит векторов\")\n",
        "\n",
        "            print(f\"\\nМодель успешно загружена!\")\n",
        "            print(f\"Количество слов: {len(self.model.index_to_key)}\")\n",
        "            print(f\"Размерность векторов: {self.model.vector_size}\")\n",
        "            print(\"Примеры слов:\", self.model.index_to_key[:5])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nОшибка загрузки модели: {e}\")\n",
        "            print(\"\\nДиагностика:\")\n",
        "            # Попробуем прочитать первые строки файла\n",
        "            try:\n",
        "                with open(MODEL_FILE, 'rb') as f:\n",
        "                    head = f.read(200)\n",
        "                    print(\"Начало файла (hex):\", head[:100].hex())\n",
        "                    try:\n",
        "                        print(\"Начало файла (text):\", head[:100].decode('utf-8', errors='replace'))\n",
        "                    except:\n",
        "                        pass\n",
        "            except Exception as e:\n",
        "                print(f\"Не удалось прочитать файл: {e}\")\n",
        "\n",
        "            raise\n",
        "\n",
        "    def _is_noun(self, word):\n",
        "        \"\"\"Проверяет, является ли слово существительным и возвращает нормальную форму\"\"\"\n",
        "        if word in self.pos_cache:\n",
        "            return self.pos_cache[word]['is_noun'], self.pos_cache[word]['normal_form']\n",
        "\n",
        "        parsed = self.morph.parse(word)\n",
        "        if not parsed:\n",
        "            self.pos_cache[word] = {'is_noun': False, 'normal_form': None}\n",
        "            return False, None\n",
        "\n",
        "        # Ищем разбор с максимальной вероятностью и существительным\n",
        "        best_parse = max(parsed, key=lambda p: p.score)\n",
        "        is_noun = 'NOUN' in best_parse.tag\n",
        "        normal_form = best_parse.normal_form if is_noun else None\n",
        "\n",
        "        self.pos_cache[word] = {'is_noun': is_noun, 'normal_form': normal_form}\n",
        "        return is_noun, normal_form\n",
        "\n",
        "    def get_nouns(self):\n",
        "        \"\"\"Получение существительных с проверкой через pymorphy3 и нормализацией\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Модель не загружена\")\n",
        "\n",
        "        print(\"\\nПоиск и проверка существительных...\")\n",
        "        unique_lemmas = set()\n",
        "        vectors = []\n",
        "        nouns = []\n",
        "        skipped = 0\n",
        "        form_errors = 0\n",
        "\n",
        "        for word in tqdm(self.model.index_to_key, desc=\"Обработка\"):\n",
        "            if len(unique_lemmas) >= 10000:\n",
        "                break\n",
        "\n",
        "            if not word.endswith('_NOUN'):\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                clean_word = word[:-5]\n",
        "\n",
        "                # Морфологический анализ\n",
        "                is_noun, normal_form = self._is_noun(clean_word)\n",
        "                if not is_noun or not normal_form:\n",
        "                    skipped += 1\n",
        "                    continue\n",
        "\n",
        "                # Дополнительная проверка нормальной формы\n",
        "                if not self.morph.parse(normal_form)[0].tag.POS == 'NOUN':\n",
        "                    form_errors += 1\n",
        "                    continue\n",
        "\n",
        "                # Проверка уникальности\n",
        "                if normal_form in unique_lemmas:\n",
        "                    skipped += 1\n",
        "                    continue\n",
        "\n",
        "                # Получаем и проверяем вектор\n",
        "                vector = self.model[word]\n",
        "                if np.isnan(vector).any() or np.isinf(vector).any():\n",
        "                    raise ValueError(\"Невалидный вектор\")\n",
        "\n",
        "                # Сохраняем данные\n",
        "                unique_lemmas.add(normal_form)\n",
        "                nouns.append(normal_form)\n",
        "                vectors.append(vector)\n",
        "\n",
        "            except Exception as e:\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "        # Проверка количества найденных лемм\n",
        "        if len(unique_lemmas) < 10000:\n",
        "            raise ValueError(f\"Недостаточно уникальных существительных. Найдено: {len(unique_lemmas)}\")\n",
        "\n",
        "        print(f\"\\nРезультаты:\")\n",
        "        print(f\"Уникальных существительных: {len(unique_lemmas)}\")\n",
        "        print(f\"Всего обработано слов: {len(vectors)}\")\n",
        "        print(f\"Пропущено: {skipped}\")\n",
        "        print(f\"Ошибки нормализации: {form_errors}\")\n",
        "\n",
        "        return np.array(vectors), nouns\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        analyzer = RusVectoresAnalyzer()\n",
        "        analyzer.download_model()\n",
        "\n",
        "        selected_vectors, selected_nouns = analyzer.get_nouns()\n",
        "        print(\"\\nПервые 10 лемм:\", selected_nouns[:10])\n",
        "        print(\"Размерность векторов:\", selected_vectors.shape)\n",
        "        print(\"Уникальных лемм:\", len(set(selected_nouns)))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nОшибка: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsG1ZH5NOF9C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "from sklearn.manifold import TSNE\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import plotly.express as px\n",
        "from adjustText import adjust_text\n",
        "from gensim.models import KeyedVectors\n",
        "from scipy.stats import entropy\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "class Word2VecSemanticAnalyzer:\n",
        "    def __init__(self, vectors, words, random_state=42):\n",
        "        \"\"\"\n",
        "        Инициализация анализатора с векторами слов и их текстовыми представлениями\n",
        "\n",
        "        Параметры:\n",
        "        vectors - numpy array с векторными представлениями слов\n",
        "        words - список слов, соответствующих векторам\n",
        "        random_state - seed для воспроизводимости\n",
        "        \"\"\"\n",
        "        self.vectors = vectors\n",
        "        self.words = words\n",
        "        self.random_state = random_state\n",
        "        self.reduced_vectors = None\n",
        "        self.gmm = None\n",
        "        self.optimal_k = None\n",
        "        self.sub_level_gmms = {}\n",
        "        self.stationary_indices = defaultdict(list)\n",
        "        self.cluster_metrics = []\n",
        "        self.word_to_idx = {word: idx for idx, word in enumerate(words)}\n",
        "\n",
        "    def reduce_dimensions(self, n_components=200, variance_threshold=0.01):\n",
        "        \"\"\"Снижение размерности с PCA и автоматическим выбором компонент\"\"\"\n",
        "        pca = PCA(n_components=min(n_components, self.vectors.shape[1]),\n",
        "                 random_state=self.random_state)\n",
        "        self.reduced_vectors = pca.fit_transform(self.vectors)\n",
        "\n",
        "        # Автоматический выбор числа компонент по порогу дисперсии\n",
        "        cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
        "        optimal_components = np.where(pca.explained_variance_ratio_ > variance_threshold)[0].size\n",
        "        optimal_components = max(100, optimal_components)  # Не меньше 100 компонент\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.subplot(121)\n",
        "        plt.plot(pca.explained_variance_ratio_, 'o-')\n",
        "        plt.axvline(optimal_components, color='r', linestyle='--')\n",
        "        plt.title('Объясненная дисперсия по компонентам')\n",
        "        plt.xlabel('Номер компоненты')\n",
        "        plt.ylabel('Доля объясненной дисперсии')\n",
        "\n",
        "        plt.subplot(122)\n",
        "        plt.plot(cum_var, 'o-')\n",
        "        plt.axvline(optimal_components, color='r', linestyle='--')\n",
        "        plt.axhline(cum_var[optimal_components], color='g', linestyle='--')\n",
        "        plt.title('Накопленная объясненная дисперсия')\n",
        "        plt.xlabel('Число компонент')\n",
        "        plt.ylabel('Накопленная дисперсия')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Если нашли лучшее число компонент - пересчитываем\n",
        "        if optimal_components != n_components:\n",
        "            print(f\"Оптимальное число компонент: {optimal_components}\")\n",
        "            pca = PCA(n_components=optimal_components, random_state=self.random_state)\n",
        "            self.reduced_vectors = pca.fit_transform(self.vectors)\n",
        "\n",
        "        return self.reduced_vectors\n",
        "\n",
        "    def find_optimal_clusters(self, max_k=50, min_k=10, step=5, n_init=5):\n",
        "        \"\"\"Определение оптимального числа кластеров с улучшенной проверкой данных\"\"\"\n",
        "        cluster_range = range(min_k, max_k + 1, step)\n",
        "\n",
        "        # Проверка входных данных\n",
        "        if self.reduced_vectors is None:\n",
        "            raise ValueError(\"Сначала выполните снижение размерности!\")\n",
        "\n",
        "        if np.isnan(self.reduced_vectors).any() or np.isinf(self.reduced_vectors).any():\n",
        "            raise ValueError(\"Обнаружены NaN/Inf значения в данных. Проверьте векторы.\")\n",
        "\n",
        "        metrics = {\n",
        "            'silhouette': [],\n",
        "            'davies_bouldin': [],\n",
        "            'combo': [],\n",
        "            'bic': [],\n",
        "            'aic': []\n",
        "        }\n",
        "\n",
        "        best_metrics = {\n",
        "            'silhouette': -np.inf,\n",
        "            'davies_bouldin': np.inf,\n",
        "            'combo': -np.inf,\n",
        "            'k': None\n",
        "        }\n",
        "\n",
        "        for k in tqdm(cluster_range, desc=\"Кластеризация\"):\n",
        "            try:\n",
        "                gmm = GaussianMixture(\n",
        "                    n_components=k,\n",
        "                    covariance_type='diag',\n",
        "                    random_state=self.random_state,\n",
        "                    n_init=n_init,\n",
        "                    max_iter=300,\n",
        "                    tol=1e-4,\n",
        "                    reg_covar=1e-3\n",
        "                )\n",
        "                gmm.fit(self.reduced_vectors)\n",
        "\n",
        "                # Проверка сходимости\n",
        "                if not gmm.converged_:\n",
        "                    raise ValueError(\"GMM не сошелся\")\n",
        "\n",
        "                labels = gmm.predict(self.reduced_vectors)\n",
        "\n",
        "                # Вычисление метрик\n",
        "                sil_score = silhouette_score(self.reduced_vectors, labels)\n",
        "                db_score = davies_bouldin_score(self.reduced_vectors, labels)\n",
        "                combo_score = 0.7*sil_score - 0.3*db_score\n",
        "\n",
        "                # Обновление лучшего результата\n",
        "                if combo_score > best_metrics['combo']:\n",
        "                    best_metrics.update({\n",
        "                        'silhouette': sil_score,\n",
        "                        'davies_bouldin': db_score,\n",
        "                        'combo': combo_score,\n",
        "                        'k': k\n",
        "                    })\n",
        "                    self.optimal_k = k\n",
        "                    self.gmm = gmm\n",
        "\n",
        "                # Сохраняем метрики\n",
        "                metrics['silhouette'].append(sil_score)\n",
        "                metrics['davies_bouldin'].append(db_score)\n",
        "                metrics['combo'].append(combo_score)\n",
        "                metrics['bic'].append(gmm.bic(self.reduced_vectors))\n",
        "                metrics['aic'].append(gmm.aic(self.reduced_vectors))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка для k={k}: {str(e)}\")\n",
        "                for name in metrics.keys():\n",
        "                    metrics[name].append(None)\n",
        "\n",
        "\n",
        "        # Визуализация метрик\n",
        "        self._plot_cluster_metrics(cluster_range, metrics)\n",
        "\n",
        "        print(f\"\\nОптимальное число кластеров: {self.optimal_k}\")\n",
        "        print(f\"Silhouette: {best_metrics['silhouette']:.3f}\")\n",
        "        print(f\"Davies-Bouldin: {best_metrics['davies_bouldin']:.3f}\")\n",
        "\n",
        "        return self.optimal_k\n",
        "\n",
        "    def _plot_cluster_metrics(self, cluster_range, metrics):\n",
        "        \"\"\"Улучшенная визуализация с проверкой значений\"\"\"\n",
        "        plt.figure(figsize=(18, 10))\n",
        "\n",
        "        # Проверка данных для графиков\n",
        "        valid_metrics = {\n",
        "            name: [(x is not None) for x in vals]\n",
        "            for name, vals in metrics.items()\n",
        "        }\n",
        "\n",
        "        # Silhouette Score\n",
        "        plt.subplot(2, 2, 1)\n",
        "        if any(valid_metrics['silhouette']):\n",
        "            plt.plot(cluster_range, metrics['silhouette'], 'o-')\n",
        "            if self.optimal_k is not None:\n",
        "                plt.axvline(self.optimal_k, color='r', linestyle='--')\n",
        "        plt.title('Silhouette Score')\n",
        "\n",
        "        # Davies-Bouldin Index\n",
        "        plt.subplot(2, 2, 2)\n",
        "        if any(valid_metrics['davies_bouldin']):\n",
        "            plt.plot(cluster_range, metrics['davies_bouldin'], 'o-', color='orange')\n",
        "            if self.optimal_k is not None:\n",
        "                plt.axvline(self.optimal_k, color='r', linestyle='--')\n",
        "        plt.title('Davies-Bouldin Index')\n",
        "\n",
        "        # Комбинированная метрика\n",
        "        plt.subplot(2, 2, 3)\n",
        "        if any(valid_metrics['combo']):\n",
        "            plt.plot(cluster_range, metrics['combo'], 'o-', color='green')\n",
        "            if self.optimal_k is not None:\n",
        "                plt.axvline(self.optimal_k, color='r', linestyle='--')\n",
        "        plt.title('Комбинированная метрика')\n",
        "\n",
        "        # Информационные критерии\n",
        "        plt.subplot(2, 2, 4)\n",
        "        if any(valid_metrics['bic']) and any(valid_metrics['aic']):\n",
        "            plt.plot(cluster_range, metrics['bic'], 'o-', color='purple', label='BIC')\n",
        "            plt.plot(cluster_range, metrics['aic'], 'o-', color='brown', label='AIC')\n",
        "            if self.optimal_k is not None:\n",
        "                plt.axvline(self.optimal_k, color='r', linestyle='--')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_clusters(self, max_points=1000, perplexity=30):\n",
        "        \"\"\"Улучшенная визуализация кластеров\"\"\"\n",
        "        if self.gmm is None:\n",
        "            raise ValueError(\"Сначала выполните кластеризацию!\")\n",
        "\n",
        "        labels = self.gmm.predict(self.reduced_vectors)\n",
        "        probs = self.gmm.predict_proba(self.reduced_vectors)\n",
        "        uncertainties = 1 - np.max(probs, axis=1)\n",
        "\n",
        "        # Выбор подмножества для визуализации\n",
        "        if len(self.reduced_vectors) > max_points:\n",
        "            # Выбираем точки с наибольшей уверенностью кластеризации\n",
        "            confident_indices = np.argsort(-uncertainties)[:max_points]\n",
        "            sample_vectors = self.reduced_vectors[confident_indices]\n",
        "            sample_words = [self.words[i] for i in confident_indices]\n",
        "            sample_labels = labels[confident_indices]\n",
        "            sample_uncertainties = uncertainties[confident_indices]\n",
        "        else:\n",
        "            sample_vectors = self.reduced_vectors\n",
        "            sample_words = self.words\n",
        "            sample_labels = labels\n",
        "            sample_uncertainties = uncertainties\n",
        "\n",
        "        # t-SNE проекция с адаптивным perplexity\n",
        "        perplexity = min(perplexity, len(sample_vectors) // 3 - 1)\n",
        "        tsne = TSNE(\n",
        "            n_components=2,\n",
        "            random_state=self.random_state,\n",
        "            perplexity=perplexity,\n",
        "            init='pca',\n",
        "            learning_rate='auto'\n",
        "        )\n",
        "        tsne_vectors = tsne.fit_transform(sample_vectors)\n",
        "\n",
        "        # Интерактивная визуализация с Plotly (с неопределенностью)\n",
        "        fig = px.scatter(\n",
        "            x=tsne_vectors[:, 0], y=tsne_vectors[:, 1],\n",
        "            color=sample_labels.astype(str),\n",
        "            size=1-sample_uncertainties,\n",
        "            hover_name=sample_words,\n",
        "            title=f\"Семантические кластеры (k={self.optimal_k})<br>Размер точки отражает уверенность кластеризации\",\n",
        "            width=1000, height=800,\n",
        "            color_discrete_sequence=px.colors.qualitative.Alphabet\n",
        "        )\n",
        "        fig.update_traces(\n",
        "            marker=dict(line=dict(width=0.5, color='DarkSlateGrey')),\n",
        "            selector=dict(mode='markers')\n",
        "        )\n",
        "        fig.show()\n",
        "\n",
        "        # Статическая визуализация с подписями\n",
        "        self._plot_static_clusters(tsne_vectors, sample_labels, sample_words, sample_uncertainties)\n",
        "\n",
        "    def _plot_static_clusters(self, tsne_vectors, labels, words, uncertainties):\n",
        "        \"\"\"Улучшенная статическая визуализация с подписями\"\"\"\n",
        "        plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # Цвета по кластерам, размер по уверенности\n",
        "        scatter = plt.scatter(\n",
        "            tsne_vectors[:, 0], tsne_vectors[:, 1],\n",
        "            c=labels, cmap='tab20',\n",
        "            s=100*(1-uncertainties),\n",
        "            alpha=0.7,\n",
        "            edgecolor='k',\n",
        "            linewidths=0.5\n",
        "        )\n",
        "\n",
        "        # Добавляем подписи для части точек (выбираем наиболее типичные)\n",
        "        texts = []\n",
        "        cluster_centers = {}\n",
        "\n",
        "        # Находим центры кластеров в t-SNE пространстве\n",
        "        for cluster in np.unique(labels):\n",
        "            mask = labels == cluster\n",
        "            center = np.median(tsne_vectors[mask], axis=0)\n",
        "            cluster_centers[cluster] = center\n",
        "\n",
        "            # Выбираем ближайшие точки к центру\n",
        "            distances = np.linalg.norm(tsne_vectors[mask] - center, axis=1)\n",
        "            closest_idx = np.argsort(distances)[:5]  # 5 ближайших слов\n",
        "            closest_indices = np.where(mask)[0][closest_idx]\n",
        "\n",
        "            for idx in closest_indices:\n",
        "                texts.append(plt.text(\n",
        "                    tsne_vectors[idx, 0], tsne_vectors[idx, 1],\n",
        "                    words[idx], fontsize=9,\n",
        "                    bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1)\n",
        "                ))\n",
        "\n",
        "        adjust_text(\n",
        "            texts,\n",
        "            arrowprops=dict(arrowstyle='-', color='gray', lw=0.5),\n",
        "            expand_points=(1.5, 1.5),\n",
        "            expand_text=(1.2, 1.2)\n",
        "        )\n",
        "        plt.title(f\"Семантические кластеры (k={self.optimal_k})\\nРазмер точки отражает уверенность кластеризации\", pad=20)\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Добавляем легенду с номерами кластеров\n",
        "        handles, _ = scatter.legend_elements()\n",
        "        plt.legend(\n",
        "            handles,\n",
        "            [f\"Кластер {i}\" for i in np.unique(labels)],\n",
        "            title=\"Кластеры\",\n",
        "            bbox_to_anchor=(1, 1),\n",
        "            loc='upper left'\n",
        "        )\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def hierarchical_clustering(self, max_subclusters=10, min_subclusters=3,\n",
        "                           max_points=200, min_cluster_size=20):\n",
        "        cluster_sizes = np.bincount(self.gmm.predict(self.reduced_vectors))\n",
        "\n",
        "        for cluster_idx in range(self.optimal_k):\n",
        "            size = cluster_sizes[cluster_idx]\n",
        "            if size < min_cluster_size:\n",
        "                print(f\"Кластер {cluster_idx} слишком мал ({size} точек), пропускаем\")\n",
        "                continue\n",
        "\n",
        "            cluster_mask = self.gmm.predict(self.reduced_vectors) == cluster_idx\n",
        "            cluster_points = self.reduced_vectors[cluster_mask]\n",
        "\n",
        "            if np.isnan(cluster_points).any():\n",
        "                print(f\"Кластер {cluster_idx} содержит NaN значения, пропускаем\")\n",
        "                continue\n",
        "\n",
        "            self._process_single_cluster(\n",
        "                cluster_idx,\n",
        "                max_subclusters,\n",
        "                min_subclusters,\n",
        "                max_points\n",
        "            )\n",
        "\n",
        "    def _process_single_cluster(self, cluster_idx, max_subclusters, min_subclusters, max_points):\n",
        "        \"\"\"Обработка одного кластера верхнего уровня с улучшенным анализом\"\"\"\n",
        "        cluster_mask = self.gmm.predict(self.reduced_vectors) == cluster_idx\n",
        "        cluster_points = self.reduced_vectors[cluster_mask]\n",
        "        cluster_words = [self.words[i] for i in np.where(cluster_mask)[0]]\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Анализ кластера {cluster_idx} ({len(cluster_points)} слов)\")\n",
        "        print(f\"Примеры слов: {', '.join(np.random.choice(cluster_words, min(10, len(cluster_words))))}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        if len(cluster_points) < min_subclusters:\n",
        "            print(\"Слишком мало точек для подкластеризации\")\n",
        "            return\n",
        "\n",
        "        # Определение оптимального числа подкластеров\n",
        "        optimal_sub_k, metrics = self._find_optimal_subclusters(\n",
        "            cluster_points, max_subclusters, min_subclusters\n",
        "        )\n",
        "\n",
        "        if optimal_sub_k < 2:\n",
        "            print(\"Не удалось найти значимые подкластеры\")\n",
        "            return\n",
        "\n",
        "        # Кластеризация и визуализация\n",
        "        gmm_sub, sub_labels, original_indices = self._perform_subclustering(\n",
        "        cluster_points, cluster_words, cluster_idx, optimal_sub_k, max_points\n",
        "        )\n",
        "\n",
        "        self._store_subcluster_results(cluster_idx, cluster_mask, sub_labels, gmm_sub, original_indices)\n",
        "\n",
        "        # Визуализация метрик\n",
        "        self._plot_subcluster_metrics(metrics, optimal_sub_k)\n",
        "\n",
        "        # Анализ семантической когерентности подкластеров\n",
        "        self._analyze_subclusters_semantics(cluster_words, sub_labels)\n",
        "\n",
        "    def _find_optimal_subclusters(self, points, max_k, min_k):\n",
        "        \"\"\"Улучшенный метод определения оптимального числа подкластеров\"\"\"\n",
        "        sub_range = range(min_k, max_k+1)\n",
        "        metrics = {\n",
        "            'k': [],\n",
        "            'silhouette': [],\n",
        "            'davies_bouldin': [],\n",
        "            'bic': [],\n",
        "            'combo': [],\n",
        "            'gap': []\n",
        "        }\n",
        "\n",
        "        best_k = min_k\n",
        "        best_combo = -np.inf\n",
        "        prev_sil = -1\n",
        "\n",
        "        for k in sub_range:\n",
        "            metrics['k'].append(k)\n",
        "            if len(points) < k*2:\n",
        "                print(f\"Слишком мало точек ({len(points)}) для k={k}\")\n",
        "                for m in ['silhouette', 'davies_bouldin', 'bic', 'combo', 'gap']:\n",
        "                    metrics[m].append(None)\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                gmm = GaussianMixture(\n",
        "                    n_components=k,\n",
        "                    covariance_type='diag',\n",
        "                    random_state=self.random_state,\n",
        "                    n_init=5,\n",
        "                    reg_covar=1e-1,\n",
        "                    tol=1e-4,\n",
        "                    max_iter=500\n",
        "                )\n",
        "                gmm.fit(points)\n",
        "                labels = gmm.predict(points)\n",
        "\n",
        "                unique_labels = np.unique(labels)\n",
        "                if len(unique_labels) < k:\n",
        "                    raise ValueError(f\"Обнаружены пустые кластеры для k={k}\")\n",
        "\n",
        "                sil_score = silhouette_score(points, labels) if k > 1 else 0\n",
        "                db_score = davies_bouldin_score(points, labels) if k > 1 else np.inf\n",
        "                bic = gmm.bic(points)\n",
        "                combo_score = 0.7*sil_score - 0.3*db_score\n",
        "\n",
        "                try:\n",
        "                    ref_disps = []\n",
        "                    for _ in range(3):\n",
        "                        # Генерация синтетических данных\n",
        "                        random_data = np.random.rand(*points.shape) * (points.max(0)-points.min(0)) + points.min(0)\n",
        "\n",
        "                        # Фильтрация NaN/Inf\n",
        "                        random_data = np.nan_to_num(random_data, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "                        # Вычисление логарифма с защитой\n",
        "                        gmm_ref = GaussianMixture(\n",
        "                            n_components=k,\n",
        "                            random_state=self.random_state,\n",
        "                            reg_covar=1e-1\n",
        "                        )\n",
        "                        gmm_ref.fit(random_data)\n",
        "                        log_prob = gmm_ref.score(random_data)\n",
        "                        ref_disps.append(log_prob if log_prob > -np.inf else 0)\n",
        "\n",
        "                    current_score = gmm.score(points)\n",
        "                    gap = np.mean(ref_disps) - (current_score if current_score > -np.inf else 0)\n",
        "                except Exception as e:\n",
        "                    print(f\"Ошибка при вычислении Gap Statistic: {str(e)}\")\n",
        "                    gap = 0\n",
        "\n",
        "                metrics['silhouette'].append(sil_score)\n",
        "                metrics['davies_bouldin'].append(db_score)\n",
        "                metrics['bic'].append(bic)\n",
        "                metrics['combo'].append(combo_score)\n",
        "                metrics['gap'].append(gap)\n",
        "\n",
        "                # Обновление лучшего k\n",
        "                if combo_score > best_combo and gap > 0:\n",
        "                    best_combo = combo_score\n",
        "                    best_k = k\n",
        "\n",
        "                prev_sil = sil_score\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка для k={k}: {str(e)}\")\n",
        "                for m in ['silhouette', 'davies_bouldin', 'bic', 'combo', 'gap']:\n",
        "                    metrics[m].append(None)\n",
        "\n",
        "        return best_k, metrics\n",
        "\n",
        "    def _perform_subclustering(self, points, words, cluster_idx, optimal_k, max_points=500):\n",
        "        \"\"\"Улучшенная визуализация подкластеров с подписями всех точек\"\"\"\n",
        "        gmm_sub = GaussianMixture(\n",
        "                n_components=optimal_k,\n",
        "                covariance_type='diag',\n",
        "                random_state=self.random_state,\n",
        "                n_init=5,\n",
        "                reg_covar=1e-1,\n",
        "                tol=1e-4,\n",
        "                max_iter=500\n",
        "            )\n",
        "        sub_labels = gmm_sub.fit_predict(points)\n",
        "\n",
        "        # Проверка на NaN в метках\n",
        "        if np.isnan(sub_labels).any():\n",
        "            raise ValueError(\"Обнаружены NaN в метках кластеров\")\n",
        "\n",
        "        # Добавляем расчет неопределенности\n",
        "        probs = gmm_sub.predict_proba(points)\n",
        "        uncertainties = 1 - np.max(probs, axis=1)\n",
        "\n",
        "        # Сохраняем оригинальные индексы до выборки\n",
        "        original_indices = np.arange(len(points))\n",
        "\n",
        "        # Ограничиваем количество точек для визуализации\n",
        "        if len(points) > max_points:\n",
        "            indices = np.random.choice(len(points), max_points, replace=False)\n",
        "            points = points[indices]\n",
        "            words = [words[i] for i in indices]\n",
        "            sub_labels = sub_labels[indices]\n",
        "            uncertainties = uncertainties[indices]\n",
        "            original_indices = original_indices[indices]\n",
        "\n",
        "        # t-SNE проекция\n",
        "        perplexity = min(50, len(points)-1)\n",
        "        tsne = TSNE(\n",
        "            n_components=2,\n",
        "            perplexity=perplexity,\n",
        "            random_state=self.random_state,\n",
        "            init='pca',\n",
        "            learning_rate='auto'\n",
        "        )\n",
        "        embedded = tsne.fit_transform(points)\n",
        "\n",
        "        # Создаем фигуру\n",
        "        plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # Размер точек зависит от уверенности кластеризации (1 - uncertainty)\n",
        "        sizes = 50 + 150 * (1 - uncertainties)\n",
        "\n",
        "        # Цвета для подкластеров\n",
        "        colors = plt.cm.tab20(np.linspace(0, 1, optimal_k))\n",
        "\n",
        "        # Рисуем точки с подписями\n",
        "        texts = []\n",
        "        for i in range(len(points)):\n",
        "            plt.scatter(\n",
        "                embedded[i, 0], embedded[i, 1],\n",
        "                color=colors[sub_labels[i]],\n",
        "                s=sizes[i],\n",
        "                alpha=0.7,\n",
        "                edgecolor='k',\n",
        "                linewidths=0.3\n",
        "            )\n",
        "            texts.append(plt.text(\n",
        "                embedded[i, 0], embedded[i, 1],\n",
        "                words[i],\n",
        "                fontsize=8,\n",
        "                color='black',\n",
        "                ha='center', va='center'\n",
        "            ))\n",
        "\n",
        "        # Настройка легенды\n",
        "        legend_elements = [\n",
        "            plt.Line2D([0], [0], marker='o', color='w',\n",
        "                      markerfacecolor=colors[i], markersize=10,\n",
        "                      label=f'Подкластер {i}')\n",
        "            for i in range(optimal_k)\n",
        "        ]\n",
        "\n",
        "        plt.legend(\n",
        "            handles=legend_elements,\n",
        "            title='Подкластеры',\n",
        "            bbox_to_anchor=(1.05, 1),\n",
        "            loc='upper left'\n",
        "        )\n",
        "\n",
        "        # Настройка заголовка\n",
        "        plt.title(\n",
        "            f\"Разбиение кластера {cluster_idx} на {optimal_k} подкластеров\\n\"\n",
        "            f\"Размер точки отражает уверенность кластеризации\\n\"\n",
        "            f\"Всего точек: {len(points)}\",\n",
        "            pad=20\n",
        "        )\n",
        "\n",
        "        # Автоматическая регулировка подписей\n",
        "        adjust_text(\n",
        "            texts,\n",
        "            arrowprops=dict(arrowstyle='-', color='gray', lw=0.5),\n",
        "            expand_points=(1.2, 1.2),\n",
        "            expand_text=(1.1, 1.1),\n",
        "            force_text=(0.5, 0.5),\n",
        "            force_points=(0.8, 0.8),\n",
        "            lim=1000\n",
        "        )\n",
        "\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return gmm_sub, sub_labels, original_indices\n",
        "\n",
        "    def _plot_subcluster_metrics(self, metrics, optimal_k):\n",
        "        \"\"\"Исправленная визуализация метрик подкластеров\"\"\"\n",
        "        plt.figure(figsize=(15,10))\n",
        "\n",
        "        # Фильтруем невалидные значения\n",
        "        valid = ~np.isnan(metrics['silhouette'])\n",
        "        k_values = np.array(metrics['k'])[valid]\n",
        "\n",
        "        plt.subplot(2,2,1)\n",
        "        plt.plot(k_values, np.array(metrics['silhouette'])[valid], 'o-')\n",
        "        plt.axvline(optimal_k, color='r', linestyle='--')\n",
        "        plt.title('Silhouette Score')\n",
        "\n",
        "        plt.subplot(2,2,2)\n",
        "        plt.plot(k_values, np.array(metrics['davies_bouldin'])[valid], 'o-', color='orange')\n",
        "        plt.axvline(optimal_k, color='r', linestyle='--')\n",
        "        plt.title('Davies-Bouldin Index')\n",
        "\n",
        "        plt.subplot(2,2,3)\n",
        "        plt.plot(k_values, np.array(metrics['combo'])[valid], 'o-', color='green')\n",
        "        plt.axvline(optimal_k, color='r', linestyle='--')\n",
        "        plt.title('Комбинированная метрика')\n",
        "\n",
        "        plt.subplot(2,2,4)\n",
        "        plt.plot(k_values, np.array(metrics['gap'])[valid], 'o-', color='purple')\n",
        "        plt.axvline(optimal_k, color='r', linestyle='--')\n",
        "        plt.title('Gap Statistic')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def _analyze_subclusters_semantics(self, words, labels):\n",
        "        \"\"\"Анализ семантической когерентности подкластеров\"\"\"\n",
        "        unique_labels = np.unique(labels)\n",
        "        print(\"\\nСемантический анализ подкластеров:\")\n",
        "\n",
        "        for sub_id in unique_labels:\n",
        "            cluster_words = [w for w, l in zip(words, labels) if l == sub_id]\n",
        "            print(f\"\\nПодкластер {sub_id} ({len(cluster_words)} слов):\")\n",
        "            print(\", \".join(cluster_words[:15]) + (\"...\" if len(cluster_words) > 15 else \"\"))\n",
        "\n",
        "    def _store_subcluster_results(self, cluster_idx, mask, sub_labels, gmm_sub, original_indices):\n",
        "        \"\"\"Сохранение результатов подкластеризации\"\"\"\n",
        "        self.sub_level_gmms[cluster_idx] = gmm_sub\n",
        "        # Получаем глобальные индексы для всего кластера\n",
        "        cluster_global_indices = np.where(mask)[0]\n",
        "\n",
        "        for sub_id in range(gmm_sub.n_components):\n",
        "            # Маска для текущего подкластера в ограниченной выборке\n",
        "            sub_mask = sub_labels == sub_id\n",
        "            # Преобразуем в глобальные индексы\n",
        "            global_indices = cluster_global_indices[original_indices[sub_mask]]\n",
        "            for idx in global_indices:\n",
        "                self.stationary_indices[idx].append((cluster_idx, sub_id))\n",
        "\n",
        "    def get_cluster_words(self, top_n=10, sort_clusters=True):\n",
        "        \"\"\"Получение топ-слов для каждого кластера с учетом вероятностей\"\"\"\n",
        "        cluster_words = defaultdict(list)\n",
        "        probs = self.gmm.predict_proba(self.reduced_vectors)\n",
        "\n",
        "        for word, label, prob in zip(self.words, self.gmm.predict(self.reduced_vectors), probs):\n",
        "            cluster_words[label].append((word, np.max(prob)))\n",
        "\n",
        "        # Сортируем слова по вероятности принадлежности к кластеру\n",
        "        result = {}\n",
        "        for k in cluster_words:\n",
        "            sorted_words = sorted(cluster_words[k], key=lambda x: -x[1])\n",
        "            result[k] = [w[0] for w in sorted_words[:top_n]]\n",
        "\n",
        "        return dict(sorted(result.items())) if sort_clusters else result\n",
        "\n",
        "    def print_cluster_words(self, top_n=15):\n",
        "        \"\"\"Вывод топ-слов с информацией о размере кластера\"\"\"\n",
        "        cluster_words = self.get_cluster_words(top_n=top_n)\n",
        "        labels = self.gmm.predict(self.reduced_vectors)\n",
        "        cluster_sizes = np.bincount(labels)\n",
        "\n",
        "        print(\"\\nТоп слов по кластерам (размер | средняя вероятность):\")\n",
        "        for cluster, words in cluster_words.items():\n",
        "            probas = self.gmm.predict_proba(self.reduced_vectors)[labels == cluster]\n",
        "            avg_prob = np.mean(np.max(probas, axis=1))\n",
        "\n",
        "            print(f\"\\nКластер {cluster} [размер: {cluster_sizes[cluster]} | prob: {avg_prob:.2f}]:\")\n",
        "            print(\", \".join(words[:top_n]))\n",
        "\n",
        "    def analyze_cluster_quality(self, sample_size=1000):\n",
        "        \"\"\"Улучшенный анализ семантической когерентности кластеров\"\"\"\n",
        "        try:\n",
        "            # Создаем временную модель для вычисления схожести\n",
        "            temp_model = KeyedVectors(vector_size=self.vectors.shape[1])\n",
        "            temp_model.add_vectors(self.words, self.vectors)\n",
        "\n",
        "            cluster_coherence = {}\n",
        "            cluster_diversity = {}\n",
        "            labels = self.gmm.predict(self.reduced_vectors)\n",
        "            cluster_sizes = np.bincount(labels)  # Вычисляем размеры кластеров\n",
        "\n",
        "            for cluster in np.unique(labels):\n",
        "                mask = labels == cluster\n",
        "                cluster_words = [self.words[i] for i in np.where(mask)[0]]\n",
        "\n",
        "                if len(cluster_words) < 2:\n",
        "                    cluster_coherence[cluster] = 0\n",
        "                    cluster_diversity[cluster] = 0\n",
        "                    continue\n",
        "\n",
        "                # Вычисляем когерентность (средняя попарная схожесть)\n",
        "                similarities = []\n",
        "                for i in range(min(len(cluster_words), sample_size)):\n",
        "                    for j in range(i+1, min(len(cluster_words), sample_size)):\n",
        "                        try:\n",
        "                            sim = temp_model.similarity(cluster_words[i], cluster_words[j])\n",
        "                            similarities.append(sim)\n",
        "                        except:\n",
        "                            continue\n",
        "\n",
        "                cluster_coherence[cluster] = np.mean(similarities) if similarities else 0\n",
        "\n",
        "                # Вычисляем диверсификацию (энтропия по топ-ближайшим словам)\n",
        "                diversities = []\n",
        "                for word in cluster_words[:sample_size]:\n",
        "                    try:\n",
        "                        similars = temp_model.most_similar(word, topn=10)\n",
        "                        similar_words = [w for w, _ in similars]\n",
        "                        word_counts = [cluster_words.count(w) for w in similar_words]\n",
        "                        word_probs = np.array(word_counts) / len(cluster_words)\n",
        "                        word_probs = word_probs[word_probs > 0]\n",
        "                        diversities.append(entropy(word_probs))\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                cluster_diversity[cluster] = np.mean(diversities) if diversities else 0\n",
        "\n",
        "            self._plot_cluster_quality(cluster_coherence, cluster_diversity, cluster_sizes)\n",
        "\n",
        "            return {\n",
        "                'coherence': cluster_coherence,\n",
        "                'diversity': cluster_diversity,\n",
        "                'cluster_sizes': cluster_sizes\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка анализа когерентности: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _plot_cluster_quality(self, coherence, diversity, cluster_sizes):\n",
        "        \"\"\"Визуализация качества кластеров с размерами\"\"\"\n",
        "        clusters = sorted(coherence.keys())\n",
        "        coh_values = [coherence[c] for c in clusters]\n",
        "        div_values = [diversity[c] for c in clusters]\n",
        "        size_values = [cluster_sizes[c] for c in clusters]\n",
        "\n",
        "        plt.figure(figsize=(18, 6))\n",
        "\n",
        "        # График когерентности с размерами\n",
        "        plt.subplot(131)\n",
        "        bars = plt.bar(clusters, coh_values, alpha=0.7)\n",
        "        plt.title('Семантическая когерентность\\n(высота - метрика, цвет - размер)')\n",
        "        plt.xlabel('Номер кластера')\n",
        "        plt.ylabel('Средняя попарная схожесть')\n",
        "\n",
        "        # Добавляем аннотации с размерами и цветовую индикацию\n",
        "        max_size = max(size_values)\n",
        "        for bar, size in zip(bars, size_values):\n",
        "            bar.set_color(plt.cm.viridis(size / max_size))\n",
        "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
        "                    f'{size}', ha='center', va='bottom')\n",
        "\n",
        "        # График диверсификации\n",
        "        plt.subplot(132)\n",
        "        plt.bar(clusters, div_values, color='orange', alpha=0.7)\n",
        "        plt.title('Семантическая диверсификация')\n",
        "        plt.xlabel('Номер кластера')\n",
        "        plt.ylabel('Энтропия ближайших слов')\n",
        "\n",
        "        # График размеров кластеров\n",
        "        plt.subplot(133)\n",
        "        plt.bar(clusters, size_values, color='green', alpha=0.7)\n",
        "        plt.title('Размеры кластеров')\n",
        "        plt.xlabel('Номер кластера')\n",
        "        plt.ylabel('Количество слов')\n",
        "        plt.yscale('log')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Совмещенный scatter plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        scatter = plt.scatter(\n",
        "            coh_values, div_values,\n",
        "            c=size_values, cmap='viridis',\n",
        "            s=np.sqrt(size_values)*10, alpha=0.7\n",
        "        )\n",
        "        plt.colorbar(scatter, label='Размер кластера')\n",
        "        plt.title('Соотношение качества кластеров\\n(Размер точки = количество слов)')\n",
        "        plt.xlabel('Когерентность')\n",
        "        plt.ylabel('Диверсификация')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Добавляем номера кластеров\n",
        "        for i, (x, y) in enumerate(zip(coh_values, div_values)):\n",
        "            plt.text(x, y, str(clusters[i]), fontsize=8,\n",
        "                    ha='center', va='center', alpha=0.7)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def analyze_covariance_diff(self, cluster_idx1, cluster_idx2, top_n=5, top_k=10):\n",
        "        \"\"\"Улучшенный анализ различий между кластерами\"\"\"\n",
        "        if cluster_idx1 not in self.sub_level_gmms or cluster_idx2 not in self.sub_level_gmms:\n",
        "            raise ValueError(\"Указанные кластеры не существуют или не были подкластеризованы\")\n",
        "\n",
        "        gmm1 = self.sub_level_gmms[cluster_idx1]\n",
        "        gmm2 = self.sub_level_gmms[cluster_idx2]\n",
        "\n",
        "        # Получаем точки для каждого кластера\n",
        "        mask1 = self.gmm.predict(self.reduced_vectors) == cluster_idx1\n",
        "        mask2 = self.gmm.predict(self.reduced_vectors) == cluster_idx2\n",
        "        points1 = self.reduced_vectors[mask1]\n",
        "        points2 = self.reduced_vectors[mask2]\n",
        "        words1 = [self.words[i] for i in np.where(mask1)[0]]\n",
        "        words2 = [self.words[i] for i in np.where(mask2)[0]]\n",
        "\n",
        "        # Нормализованные ковариации для сравнения\n",
        "        cov1 = normalize(gmm1.covariances_, norm='l2')\n",
        "        cov2 = normalize(gmm2.covariances_, norm='l2')\n",
        "\n",
        "        # Вычисляем KL-дивергенцию между распределениями по измерениям\n",
        "        kl_divs = []\n",
        "        for dim in range(self.reduced_vectors.shape[1]):\n",
        "            # Оцениваем распределения по измерениям\n",
        "            hist1, _ = np.histogram(points1[:, dim], bins=20, density=True)\n",
        "            hist2, _ = np.histogram(points2[:, dim], bins=20, density=True)\n",
        "\n",
        "            # Добавляем небольшое значение чтобы избежать нулей\n",
        "            hist1 = hist1 + 1e-10\n",
        "            hist2 = hist2 + 1e-10\n",
        "            hist1 = hist1 / hist1.sum()\n",
        "            hist2 = hist2 / hist2.sum()\n",
        "\n",
        "            kl = entropy(hist1, hist2) + entropy(hist2, hist1)\n",
        "            kl_divs.append(kl)\n",
        "\n",
        "        # Находим измерения с максимальными различиями\n",
        "        top_dims = np.argsort(-np.array(kl_divs))[:top_n]\n",
        "\n",
        "        # Собираем результаты\n",
        "        results = []\n",
        "        for dim in top_dims:\n",
        "            # Для каждого измерения анализируем подкластеры\n",
        "            for comp1 in range(gmm1.n_components):\n",
        "                for comp2 in range(gmm2.n_components):\n",
        "                    # Слова для компонент\n",
        "                    comp1_words = [w for w, l in zip(words1, gmm1.predict(points1)) if l == comp1]\n",
        "                    comp1_points = points1[gmm1.predict(points1) == comp1]\n",
        "                    comp2_words = [w for w, l in zip(words2, gmm2.predict(points2)) if l == comp2]\n",
        "                    comp2_points = points2[gmm2.predict(points2) == comp2]\n",
        "\n",
        "                    if len(comp1_words) == 0 or len(comp2_words) == 0:\n",
        "                        continue\n",
        "\n",
        "                    # Топ и антонимы по измерению\n",
        "                    top1 = self._get_top_words_by_dim(comp1_words, comp1_points, dim, top_k)\n",
        "                    opposite1 = self._get_top_words_by_dim(comp1_words, comp1_points, dim, top_k//2, ascending=True)\n",
        "                    top2 = self._get_top_words_by_dim(comp2_words, comp2_points, dim, top_k)\n",
        "                    opposite2 = self._get_top_words_by_dim(comp2_words, comp2_points, dim, top_k//2, ascending=True)\n",
        "\n",
        "                    # Средние значения по измерению\n",
        "                    mean1 = np.mean(comp1_points[:, dim])\n",
        "                    mean2 = np.mean(comp2_points[:, dim])\n",
        "\n",
        "                    results.append({\n",
        "                        'dimension': dim,\n",
        "                        'kl_divergence': kl_divs[dim],\n",
        "                        'cluster1': cluster_idx1,\n",
        "                        'cluster2': cluster_idx2,\n",
        "                        'component1': comp1,\n",
        "                        'component2': comp2,\n",
        "                        'mean1': mean1,\n",
        "                        'mean2': mean2,\n",
        "                        'mean_diff': abs(mean1 - mean2),\n",
        "                        'cluster1_top': top1,\n",
        "                        'cluster1_opposite': opposite1,\n",
        "                        'cluster2_top': top2,\n",
        "                        'cluster2_opposite': opposite2\n",
        "                    })\n",
        "\n",
        "        # Сортируем результаты по KL-дивергенции\n",
        "        results.sort(key=lambda x: -x['kl_divergence'])\n",
        "        return results[:top_n]\n",
        "\n",
        "    def analyze_variance(self, cluster_idx, top_n=3, top_k=10):\n",
        "        \"\"\"Улучшенный анализ дисперсий внутри кластера\"\"\"\n",
        "        if cluster_idx not in self.sub_level_gmms:\n",
        "            raise ValueError(\"Указанный кластер не существует или не был подкластеризован\")\n",
        "\n",
        "        # Получаем точки кластера\n",
        "        mask = self.gmm.predict(self.reduced_vectors) == cluster_idx\n",
        "        points = self.reduced_vectors[mask]\n",
        "        words = [self.words[i] for i in np.where(mask)[0]]\n",
        "\n",
        "        # Вычисляем относительную дисперсию (по сравнению с общим распределением)\n",
        "        global_var = np.var(self.reduced_vectors, axis=0)\n",
        "        cluster_var = np.var(points, axis=0)\n",
        "        relative_var = cluster_var / (global_var + 1e-10)\n",
        "\n",
        "        # Находим измерения с максимальной относительной дисперсией\n",
        "        top_dims = np.argsort(-relative_var)[:top_n]\n",
        "\n",
        "        # Собираем результаты\n",
        "        results = []\n",
        "        for dim in top_dims:\n",
        "            # Топ и антонимы по измерению\n",
        "            top_words = self._get_top_words_by_dim(words, points, dim, top_k)\n",
        "            opposite_words = self._get_top_words_by_dim(words, points, dim, top_k//2, ascending=True)\n",
        "\n",
        "            # Анализ корреляций с другими измерениями\n",
        "            corr_matrix = np.corrcoef(points.T)\n",
        "            corr_dims = np.argsort(-np.abs(corr_matrix[dim]))[1:top_n+1]\n",
        "\n",
        "            results.append({\n",
        "                'dimension': dim,\n",
        "                'variance': cluster_var[dim],\n",
        "                'relative_variance': relative_var[dim],\n",
        "                'cluster': cluster_idx,\n",
        "                'top_words': top_words,\n",
        "                'opposite_words': opposite_words,\n",
        "                'correlated_dims': corr_dims.tolist(),\n",
        "                'correlation_values': corr_matrix[dim, corr_dims].tolist()\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _get_top_words_by_dim(self, words, vectors, dim, n_words, ascending=False):\n",
        "        \"\"\"Вспомогательный метод для получения топ-N слов по измерению\"\"\"\n",
        "        if len(vectors) == 0:\n",
        "            return []\n",
        "\n",
        "        indices = np.argsort(vectors[:, dim])\n",
        "        if not ascending:\n",
        "            indices = indices[::-1]\n",
        "        return [words[i] for i in indices[:n_words]]\n",
        "\n",
        "    def print_covariance_analysis(self, results):\n",
        "        \"\"\"Улучшенный вывод результатов анализа ковариаций\"\"\"\n",
        "        print(\"\\nРезультаты анализа различий между кластерами:\")\n",
        "        print(\"(KL-дивергенция измеряет различия в распределениях значений по измерениям)\")\n",
        "\n",
        "        for res in results:\n",
        "            print(f\"\\nИзмерение {res['dimension']}\")\n",
        "            print(f\"KL-дивергенция: {res['kl_divergence']:.3f}\")\n",
        "            print(f\"Средние значения: кластер {res['cluster1']} = {res['mean1']:.2f}, \"\n",
        "                  f\"кластер {res['cluster2']} = {res['mean2']:.2f}, разница = {res['mean_diff']:.2f}\")\n",
        "\n",
        "            print(f\"\\nКластер {res['cluster1']} (компонента {res['component1']}):\")\n",
        "            print(f\"  Топ: {', '.join(res['cluster1_top'][:5])}\")\n",
        "            print(f\"  Антонимы: {', '.join(res['cluster1_opposite'][:3])}\")\n",
        "\n",
        "            print(f\"\\nКластер {res['cluster2']} (компонента {res['component2']}):\")\n",
        "            print(f\"  Топ: {', '.join(res['cluster2_top'][:5])}\")\n",
        "            print(f\"  Антонимы: {', '.join(res['cluster2_opposite'][:3])}\")\n",
        "\n",
        "            print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "    def print_variance_analysis(self, results):\n",
        "        \"\"\"Улучшенный вывод результатов анализа дисперсий\"\"\"\n",
        "        print(\"\\nРезультаты анализа дисперсий:\")\n",
        "        print(\"(Относительная дисперсия показывает значимость измерения в кластере по сравнению с общим распределением)\")\n",
        "\n",
        "        for res in results:\n",
        "            print(f\"\\nИзмерение {res['dimension']}\")\n",
        "            print(f\"Дисперсия: {res['variance']:.3f}\")\n",
        "            print(f\"Относительная дисперсия: {res['relative_variance']:.2f}x\")\n",
        "\n",
        "            print(f\"\\nТоп слова: {', '.join(res['top_words'][:5])}\")\n",
        "            print(f\"Антонимы: {', '.join(res['opposite_words'][:3])}\")\n",
        "\n",
        "            print(\"\\nНаиболее коррелированные измерения:\")\n",
        "            for dim, corr in zip(res['correlated_dims'], res['correlation_values']):\n",
        "                print(f\"  Измерение {dim}: коэффициент корреляции = {corr:.2f}\")\n",
        "\n",
        "            print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "    def compare_clusters_visually(self, cluster_indices, max_points=500, label_top_n=20):\n",
        "        \"\"\"Визуализация сравнения кластеров с подписями точек\"\"\"\n",
        "        if not all(idx in self.sub_level_gmms for idx in cluster_indices):\n",
        "            raise ValueError(\"Некоторые кластеры не существуют или не были подкластеризованы\")\n",
        "\n",
        "        # Собираем маску для выбранных кластеров\n",
        "        combined_mask = np.zeros(len(self.reduced_vectors), dtype=bool)\n",
        "        for idx in cluster_indices:\n",
        "            combined_mask |= (self.gmm.predict(self.reduced_vectors) == idx)\n",
        "\n",
        "        # Создаем mapper: глобальный индекс -> локальный индекс\n",
        "        index_mapper = np.cumsum(combined_mask) - 1\n",
        "        index_mapper[~combined_mask] = -1\n",
        "\n",
        "        # Подготовка данных\n",
        "        points = self.reduced_vectors[combined_mask]\n",
        "        words = [self.words[i] for i in np.where(combined_mask)[0]]\n",
        "        total_points = len(points)\n",
        "\n",
        "        # Инициализация меток\n",
        "        combined_labels = np.zeros(total_points, dtype=int)\n",
        "        offset = 0\n",
        "\n",
        "        for idx in cluster_indices:\n",
        "            # Получаем точки кластера\n",
        "            cluster_mask = (self.gmm.predict(self.reduced_vectors) == idx)\n",
        "            global_indices = np.where(cluster_mask)[0]\n",
        "            local_indices = index_mapper[global_indices]\n",
        "\n",
        "            # Фильтруем невалидные индексы\n",
        "            valid_mask = (local_indices != -1)\n",
        "            local_indices = local_indices[valid_mask]\n",
        "\n",
        "            # Получаем подкластеры\n",
        "            gmm_sub = self.sub_level_gmms[idx]\n",
        "            sub_labels = gmm_sub.predict(self.reduced_vectors[cluster_mask][valid_mask])\n",
        "\n",
        "            # Обновляем метки\n",
        "            combined_labels[local_indices] = sub_labels + offset\n",
        "            offset += gmm_sub.n_components + 1\n",
        "\n",
        "        # t-SNE проекция\n",
        "        tsne = TSNE(n_components=2, random_state=42)\n",
        "        embedded = tsne.fit_transform(points)\n",
        "\n",
        "        # Создаем фигуру\n",
        "        plt.figure(figsize=(20, 16))\n",
        "\n",
        "        # Цвета для кластеров\n",
        "        colors = plt.cm.tab20(np.linspace(0, 1, offset))\n",
        "\n",
        "        # Рисуем точки\n",
        "        scatter = plt.scatter(\n",
        "            embedded[:, 0], embedded[:, 1],\n",
        "            c=[colors[l] for l in combined_labels],\n",
        "            alpha=0.7, s=50\n",
        "        )\n",
        "\n",
        "        # Добавляем подписи для наиболее характерных точек каждого подкластера\n",
        "        texts = []\n",
        "        for label in np.unique(combined_labels):\n",
        "            mask = combined_labels == label\n",
        "            cluster_points = embedded[mask]\n",
        "            cluster_words = [words[i] for i in np.where(mask)[0]]\n",
        "\n",
        "            # Находим центр подкластера\n",
        "            center = np.median(cluster_points, axis=0)\n",
        "\n",
        "            # Выбираем ближайшие точки к центру\n",
        "            distances = np.linalg.norm(cluster_points - center, axis=1)\n",
        "            closest_indices = np.argsort(distances)[:label_top_n]\n",
        "\n",
        "            for idx in closest_indices:\n",
        "                texts.append(plt.text(\n",
        "                    cluster_points[idx, 0], cluster_points[idx, 1],\n",
        "                    cluster_words[idx],\n",
        "                    fontsize=8,\n",
        "                    bbox=dict(\n",
        "                        facecolor='white',\n",
        "                        alpha=0.7,\n",
        "                        edgecolor=colors[label],\n",
        "                        boxstyle='round,pad=0.2'\n",
        "                    )\n",
        "                ))\n",
        "\n",
        "        # Автоматическая регулировка подписей\n",
        "        adjust_text(\n",
        "            texts,\n",
        "            arrowprops=dict(\n",
        "                arrowstyle='-',\n",
        "                color='gray',\n",
        "                lw=0.5,\n",
        "                alpha=0.5\n",
        "            ),\n",
        "            expand_points=(1.2, 1.2),\n",
        "            expand_text=(1.1, 1.1),\n",
        "            force_text=(0.5, 0.5),\n",
        "            force_points=(0.8, 0.8),\n",
        "            lim=1000\n",
        "        )\n",
        "\n",
        "        # Настройка легенды\n",
        "        legend_elements = [\n",
        "            plt.Line2D([0], [0], marker='o', color='w',\n",
        "                      markerfacecolor=colors[i], markersize=10,\n",
        "                      label=f'Кластер {i}')\n",
        "            for i in np.unique(combined_labels)\n",
        "        ]\n",
        "\n",
        "        plt.legend(\n",
        "            handles=legend_elements,\n",
        "            title='Подкластеры',\n",
        "            bbox_to_anchor=(1.05, 1),\n",
        "            loc='upper left'\n",
        "        )\n",
        "\n",
        "        plt.title(\n",
        "            f\"Сравнение кластеров: {', '.join(map(str, cluster_indices))}\\n\"\n",
        "            f\"Подписаны топ-{label_top_n} слов каждого подкластера\",\n",
        "            pad=20\n",
        "        )\n",
        "        plt.grid(True, alpha=0.2)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Инициализация анализатора\n",
        "    analyzer = Word2VecSemanticAnalyzer(selected_vectors, selected_nouns)\n",
        "\n",
        "    # 1. Снижение размерности с автоматическим выбором компонент\n",
        "    analyzer.reduce_dimensions(n_components=200, variance_threshold=0.0025)\n",
        "\n",
        "    # 2. Верхнеуровневая кластеризация\n",
        "    analyzer.find_optimal_clusters(max_k=100, min_k=50, n_init=10, step=5)\n",
        "\n",
        "    # 3. Визуализация и анализ кластеров верхнего уровня\n",
        "    analyzer.visualize_clusters(max_points=2000)\n",
        "    analyzer.print_cluster_words(top_n=15)\n",
        "    quality_report = analyzer.analyze_cluster_quality()\n",
        "\n",
        "    # 4. Иерархическая подкластеризация (только для крупных кластеров)\n",
        "    analyzer.hierarchical_clustering(max_subclusters=10, min_cluster_size=60)\n",
        "\n",
        "    # 5. Углубленный анализ кластеров\n",
        "    if len(analyzer.sub_level_gmms) >= 2:\n",
        "        valid_clusters = [k for k in analyzer.sub_level_gmms.keys()]\n",
        "\n",
        "        if quality_report and 'coherence' in quality_report:\n",
        "            cluster_coherence = quality_report['coherence']\n",
        "            # Фильтруем только кластеры с подкластеризацией\n",
        "            filtered_clusters = [(k, v) for k, v in cluster_coherence.items() if k in valid_clusters]\n",
        "\n",
        "            if len(filtered_clusters) >= 2:\n",
        "                sorted_clusters = sorted(filtered_clusters, key=lambda x: x[1])\n",
        "                cluster1 = sorted_clusters[0][0]\n",
        "                cluster2 = sorted_clusters[-1][0]\n",
        "            else:\n",
        "                cluster1, cluster2 = valid_clusters[:2]\n",
        "        else:\n",
        "            cluster1, cluster2 = valid_clusters[:2]\n",
        "\n",
        "        print(f\"\\nСравниваем кластеры {cluster1} и {cluster2}\")\n",
        "        if quality_report and 'coherence' in quality_report:\n",
        "            print(f\"Кластер {cluster1} (когерентность: {quality_report['coherence'].get(cluster1, 'N/A'):.3f})\")\n",
        "            print(f\"Кластер {cluster2} (когерентность: {quality_report['coherence'].get(cluster2, 'N/A'):.3f}\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Анализ различий в ковариациях\n",
        "        try:\n",
        "            cov_results = analyzer.analyze_covariance_diff(cluster1, cluster2)\n",
        "            analyzer.print_covariance_analysis(cov_results)\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при анализе ковариаций: {str(e)}\")\n",
        "\n",
        "        # Анализ дисперсий для кластеров\n",
        "        try:\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(f\"Анализ структуры кластера {cluster1}:\")\n",
        "            var_results1 = analyzer.analyze_variance(cluster1)\n",
        "            analyzer.print_variance_analysis(var_results1)\n",
        "\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(f\"Анализ структуры кластера {cluster2}:\")\n",
        "            var_results2 = analyzer.analyze_variance(cluster2)\n",
        "            analyzer.print_variance_analysis(var_results2)\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при анализе дисперсий: {str(e)}\")\n",
        "\n",
        "        # Сравнительная визуализация\n",
        "        try:\n",
        "            analyzer.compare_clusters_visually([cluster1, cluster2])\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при визуализации: {str(e)}\")\n",
        "    else:\n",
        "        print(\"\\nНедостаточно кластеров для сравнения (нужно минимум 2 подкластеризованных кластера)\")"
      ],
      "metadata": {
        "id": "itMQy0he8GSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouCGhZMPV6Aq"
      },
      "outputs": [],
      "source": [
        "!pip install pymorphy3 nltk transformers datasets plotly scikit-learn adjustText"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://opencorpora.org/files/export/annot/annot.opcorpora.xml.zip\n",
        "!unzip annot.opcorpora.xml.zip"
      ],
      "metadata": {
        "id": "WtlMzbQ97YLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "from adjustText import adjust_text\n",
        "from scipy.stats import entropy\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from multiprocessing import Pool\n",
        "import gc\n",
        "from collections import defaultdict\n",
        "\n",
        "def find_noun_positions(sent, noun_set):\n",
        "    sent_lower = sent.lower()\n",
        "    positions = defaultdict(list)\n",
        "    for noun in noun_set:\n",
        "        for match in re.finditer(r'\\b' + re.escape(noun) + r'\\b', sent_lower):\n",
        "            positions[noun].append((match.start(), match.end()))\n",
        "    return positions\n",
        "\n",
        "class BertSemanticAnalyzer:\n",
        "    def __init__(self, model_name=\"ai-forever/ruBert-base\"):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.max_seq_length = 128\n",
        "        self.vectors = None\n",
        "        self.vocab = None\n",
        "        self.word_to_idx = None\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
        "        self.model.eval()\n",
        "        self.random_state = 42\n",
        "        self.morph = MorphAnalyzer()\n",
        "        self.pos_cache = {}\n",
        "        self.stopwords = set(stopwords.words('russian'))\n",
        "        self.punctuation = set(string.punctuation + '«»—')\n",
        "        self.sub_level_gmms = {}\n",
        "        self.stationary_indices = defaultdict(list)\n",
        "\n",
        "    def extract_sentences(self, file_path):\n",
        "        try:\n",
        "            tree = ET.parse(file_path)\n",
        "            return [text.text.strip() for text in tree.findall(\".//source\") if text.text]\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка чтения файла: {e}\")\n",
        "            return []\n",
        "\n",
        "    def extract_frequent_nouns(self, sentences, top_n=10000):\n",
        "        counter = Counter()\n",
        "        for sent in tqdm(sentences, desc=\"Извлечение существительных\"):\n",
        "            words = re.findall(r'\\b[\\w-]+\\b', sent.lower())\n",
        "            for word in words:\n",
        "                if word in self.stopwords or len(word) < 2:\n",
        "                    continue\n",
        "\n",
        "                if word not in self.pos_cache:\n",
        "                    parsed = self.morph.parse(word)\n",
        "                    if parsed:\n",
        "                        best = max(parsed, key=lambda p: p.score)\n",
        "                        self.pos_cache[word] = {\n",
        "                            'lemma': best.normal_form,\n",
        "                            'pos': best.tag.POS\n",
        "                        }\n",
        "\n",
        "                info = self.pos_cache.get(word)\n",
        "                if info and info['pos'] == 'NOUN':\n",
        "                    counter[info['lemma']] += 1\n",
        "\n",
        "        return [lemma for lemma, _ in counter.most_common(top_n)]\n",
        "\n",
        "    def get_contextual_embeddings(self, sentences, nouns, batch_size=64, max_contexts_per_noun=50, sample_size=None):\n",
        "        noun_set = set(nouns)\n",
        "\n",
        "        # Если указана подвыборка корпуса, берем случайную часть\n",
        "        if sample_size is not None and sample_size < len(sentences):\n",
        "            import random\n",
        "            random.seed(self.random_state)\n",
        "            sentences = random.sample(sentences, sample_size)\n",
        "\n",
        "        noun_pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, nouns)) + r')\\b')\n",
        "        relevant_sents = [s for s in sentences if noun_pattern.search(s.lower())]\n",
        "\n",
        "        # Предварительная индексация существительных с ограничением вхождений\n",
        "        noun_indices = defaultdict(list)\n",
        "        counts = defaultdict(int)\n",
        "\n",
        "        print(\"Индексация существительных...\")\n",
        "        with Pool() as pool:\n",
        "            results = list(\n",
        "                tqdm(\n",
        "                    pool.starmap(find_noun_positions, [(sent, noun_set) for sent in relevant_sents]),\n",
        "                    total=len(relevant_sents),\n",
        "                    desc=\"Индексация предложений\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "        for i, positions in enumerate(results):\n",
        "            for noun, spans in positions.items():\n",
        "                if counts[noun] < max_contexts_per_noun:\n",
        "                    noun_indices[noun].append((i, spans))\n",
        "                    counts[noun] += 1\n",
        "\n",
        "        # Обработка батчей\n",
        "        embeddings = defaultdict(list)\n",
        "        total_batches = (len(relevant_sents) + batch_size - 1) // batch_size\n",
        "\n",
        "        print(\"Извлечение контекстуальных эмбеддингов...\")\n",
        "        for batch_idx in tqdm(range(total_batches), desc=\"Обработка контекстов\"):\n",
        "            start = batch_idx * batch_size\n",
        "            end = (batch_idx + 1) * batch_size\n",
        "            batch = relevant_sents[start:end]\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                batch,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=self.max_seq_length,\n",
        "                return_offsets_mapping=True\n",
        "            ).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                model_inputs = {k: v for k, v in inputs.items() if k != \"offset_mapping\"}\n",
        "                outputs = self.model(**model_inputs)\n",
        "                hidden_states = outputs.last_hidden_state.cpu().numpy()\n",
        "\n",
        "            for sent_idx in range(len(batch)):\n",
        "                offset_mapping = inputs.offset_mapping[sent_idx].cpu().numpy()\n",
        "\n",
        "                for noun, positions in noun_indices.items():\n",
        "                    for global_idx, spans in positions:\n",
        "                        if global_idx != start + sent_idx:\n",
        "                            continue\n",
        "\n",
        "                        token_indices = []\n",
        "                        for start_char, end_char in spans:\n",
        "                            for tok_idx, (tok_start, tok_end) in enumerate(offset_mapping):\n",
        "                                if tok_start >= start_char and tok_end <= end_char:\n",
        "                                    token_indices.append(tok_idx)\n",
        "\n",
        "                        if token_indices:\n",
        "                            emb = hidden_states[sent_idx, token_indices].mean(axis=0)\n",
        "                            embeddings[noun].append(emb)\n",
        "\n",
        "            del inputs, outputs, hidden_states\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        self._create_final_embeddings(embeddings, nouns)\n",
        "        return self.vectors\n",
        "\n",
        "    def _create_final_embeddings(self, embeddings, nouns):\n",
        "        self.vocab = []\n",
        "        self.vectors = []\n",
        "        for noun in nouns:\n",
        "            if noun in embeddings and len(embeddings[noun]) > 0:\n",
        "                self.vocab.append(noun)\n",
        "                self.vectors.append(np.mean(embeddings[noun], axis=0))\n",
        "\n",
        "        self.vectors = np.array(self.vectors)\n",
        "        self.word_to_idx = {w: i for i, w in enumerate(self.vocab)}\n",
        "        print(f\"Получено {len(self.vectors)} контекстуальных эмбеддингов\")\n",
        "\n",
        "    def reduce_dimensions(self, n_components=200, variance_threshold=0.01):\n",
        "        pca = PCA(n_components=min(n_components, self.vectors.shape[1]),\n",
        "                random_state=self.random_state)\n",
        "        self.reduced_vectors = pca.fit_transform(self.vectors)\n",
        "\n",
        "        cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
        "        optimal_components = np.where(pca.explained_variance_ratio_ > variance_threshold)[0].size\n",
        "        optimal_components = max(150, optimal_components)\n",
        "\n",
        "        plt.figure(figsize=(12,6))\n",
        "        plt.subplot(121)\n",
        "        plt.plot(pca.explained_variance_ratio_, 'o-')\n",
        "        plt.axvline(optimal_components, color='r', linestyle='--')\n",
        "        plt.title('Объясненная дисперсия по компонентам')\n",
        "        plt.xlabel('Номер компоненты')\n",
        "        plt.ylabel('Доля объясненной дисперсии')\n",
        "\n",
        "        plt.subplot(122)\n",
        "        plt.plot(cum_var, 'o-')\n",
        "        plt.axvline(optimal_components, color='r', linestyle='--')\n",
        "        plt.axhline(cum_var[optimal_components], color='g', linestyle='--')\n",
        "        plt.title('Накопленная объясненная дисперсия')\n",
        "        plt.xlabel('Число компонент')\n",
        "        plt.ylabel('Накопленная дисперсия')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        if optimal_components != n_components:\n",
        "            print(f\"Оптимальное число компонент: {optimal_components}\")\n",
        "            pca = PCA(n_components=optimal_components, random_state=self.random_state)\n",
        "            self.reduced_vectors = pca.fit_transform(self.vectors)\n",
        "        return self.reduced_vectors\n",
        "\n",
        "\n",
        "    def find_optimal_clusters(self, max_k=150, min_k=50, step=5, n_init=5):\n",
        "        cluster_range = range(min_k, max_k + 1, step)\n",
        "        metrics = {\n",
        "            'silhouette': [],\n",
        "            'davies_bouldin': [],\n",
        "            'combo': [],\n",
        "            'bic': [],\n",
        "            'aic': []\n",
        "        }\n",
        "\n",
        "        best_metrics = {\n",
        "            'silhouette': -np.inf,\n",
        "            'davies_bouldin': np.inf,\n",
        "            'combo': -np.inf,\n",
        "            'k': None\n",
        "        }\n",
        "\n",
        "        for k in tqdm(cluster_range, desc=\"Кластеризация\"):\n",
        "            try:\n",
        "                gmm = GaussianMixture(\n",
        "                    n_components=k,\n",
        "                    covariance_type='diag',\n",
        "                    random_state=self.random_state,\n",
        "                    n_init=n_init,\n",
        "                    max_iter=500,\n",
        "                    tol=1e-4,\n",
        "                    reg_covar=1e-3\n",
        "                )\n",
        "                gmm.fit(self.reduced_vectors)\n",
        "\n",
        "                if not gmm.converged_:\n",
        "                    raise ValueError(\"GMM не сошелся\")\n",
        "\n",
        "                labels = gmm.predict(self.reduced_vectors)\n",
        "\n",
        "                sil_score = silhouette_score(self.reduced_vectors, labels)\n",
        "                db_score = davies_bouldin_score(self.reduced_vectors, labels)\n",
        "                combo_score = 0.7*sil_score - 0.3*db_score\n",
        "\n",
        "                if combo_score > best_metrics['combo']:\n",
        "                    best_metrics.update({\n",
        "                        'silhouette': sil_score,\n",
        "                        'davies_bouldin': db_score,\n",
        "                        'combo': combo_score,\n",
        "                        'k': k\n",
        "                    })\n",
        "                    self.optimal_k = k\n",
        "                    self.gmm = gmm\n",
        "\n",
        "                metrics['silhouette'].append(sil_score)\n",
        "                metrics['davies_bouldin'].append(db_score)\n",
        "                metrics['combo'].append(combo_score)\n",
        "                metrics['bic'].append(gmm.bic(self.reduced_vectors))\n",
        "                metrics['aic'].append(gmm.aic(self.reduced_vectors))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка для k={k}: {str(e)}\")\n",
        "                for name in metrics.keys():\n",
        "                    metrics[name].append(None)\n",
        "\n",
        "        self._plot_cluster_metrics(cluster_range, metrics)\n",
        "        print(f\"\\nОптимальное число кластеров: {self.optimal_k}\")\n",
        "        print(f\"Silhouette: {best_metrics['silhouette']:.3f}\")\n",
        "        print(f\"Davies-Bouldin: {best_metrics['davies_bouldin']:.3f}\")\n",
        "        return self.optimal_k\n",
        "\n",
        "    def _plot_cluster_metrics(self, cluster_range, metrics):\n",
        "        plt.figure(figsize=(18,10))\n",
        "\n",
        "        valid_metrics = {name: [(x is not None) for x in vals] for name, vals in metrics.items()}\n",
        "\n",
        "        plt.subplot(2,2,1)\n",
        "        if any(valid_metrics['silhouette']):\n",
        "            plt.plot(cluster_range, metrics['silhouette'], 'o-')\n",
        "            plt.axvline(self.optimal_k, color='r', linestyle='--')\n",
        "        plt.title('Silhouette Score')\n",
        "        plt.xlabel('Число кластеров')\n",
        "        plt.ylabel('Значение метрики')\n",
        "\n",
        "        plt.subplot(2,2,2)\n",
        "        if any(valid_metrics['davies_bouldin']):\n",
        "            plt.plot(cluster_range, metrics['davies_bouldin'], 'o-', color='orange')\n",
        "            plt.axvline(self.optimal_k, color='r', linestyle='--')\n",
        "        plt.title('Davies-Bouldin Index')\n",
        "        plt.xlabel('Число кластеров')\n",
        "        plt.ylabel('Значение метрики')\n",
        "\n",
        "        plt.subplot(2,2,3)\n",
        "        if any(valid_metrics['combo']):\n",
        "            plt.plot(cluster_range, metrics['combo'], 'o-', color='green')\n",
        "            plt.axvline(self.optimal_k, color='r', linestyle='--')\n",
        "        plt.title('Комбинированная метрика')\n",
        "        plt.xlabel('Число кластеров')\n",
        "        plt.ylabel('Значение метрики')\n",
        "\n",
        "        plt.subplot(2,2,4)\n",
        "        if any(valid_metrics['bic']) and any(valid_metrics['aic']):\n",
        "            plt.plot(cluster_range, metrics['bic'], 'o-', label='BIC')\n",
        "            plt.plot(cluster_range, metrics['aic'], 'o-', label='AIC')\n",
        "            plt.axvline(self.optimal_k, color='r', linestyle='--')\n",
        "            plt.legend()\n",
        "        plt.xlabel('Число кластеров')\n",
        "        plt.ylabel('Значение метрики')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_clusters(self, max_points=1000, perplexity=30):\n",
        "        if self.gmm is None:\n",
        "            raise ValueError(\"Сначала выполните кластеризацию!\")\n",
        "\n",
        "        labels = self.gmm.predict(self.reduced_vectors)\n",
        "        cluster_sizes = np.bincount(labels)\n",
        "        probs = self.gmm.predict_proba(self.reduced_vectors)\n",
        "        uncertainties = 1 - np.max(probs, axis=1)\n",
        "\n",
        "        if len(self.reduced_vectors) > max_points:\n",
        "            confident_indices = np.argsort(-uncertainties)[:max_points]\n",
        "            sample_vectors = self.reduced_vectors[confident_indices]\n",
        "            sample_words = [self.vocab[i] for i in confident_indices]\n",
        "            sample_labels = labels[confident_indices]\n",
        "            sample_uncertainties = uncertainties[confident_indices]\n",
        "        else:\n",
        "            sample_vectors = self.reduced_vectors\n",
        "            sample_words = self.vocab\n",
        "            sample_labels = labels\n",
        "            sample_uncertainties = uncertainties\n",
        "\n",
        "        perplexity = min(perplexity, len(sample_vectors) // 3 - 1)\n",
        "        tsne = TSNE(\n",
        "            n_components=2,\n",
        "            random_state=self.random_state,\n",
        "            perplexity=perplexity,\n",
        "            init='pca',\n",
        "            learning_rate='auto'\n",
        "        )\n",
        "        tsne_vectors = tsne.fit_transform(sample_vectors)\n",
        "\n",
        "        fig = px.scatter(\n",
        "            x=tsne_vectors[:, 0], y=tsne_vectors[:, 1],\n",
        "            color=sample_labels.astype(str),\n",
        "            size=1-sample_uncertainties,\n",
        "            hover_name=sample_words,\n",
        "            title=(\n",
        "                f\"Семантические кластеры (k={self.optimal_k})<br>\"\n",
        "                f\"Всего слов: {len(self.vocab)} | Средний размер кластера: {np.mean(cluster_sizes):.1f}<br>\"\n",
        "                f\"Размер точки отражает уверенность кластеризации\"\n",
        "            ),\n",
        "            width=1000, height=800,\n",
        "            color_discrete_sequence=px.colors.qualitative.Alphabet\n",
        "        )\n",
        "        fig.update_traces(marker=dict(line=dict(width=0.5, color='DarkSlateGrey')))\n",
        "        fig.show()\n",
        "\n",
        "        self._plot_static_clusters(tsne_vectors, sample_labels, sample_words, sample_uncertainties)\n",
        "\n",
        "    def _plot_static_clusters(self, tsne_vectors, labels, words, uncertainties):\n",
        "        plt.figure(figsize=(20,15))\n",
        "        scatter = plt.scatter(\n",
        "            tsne_vectors[:,0], tsne_vectors[:,1],\n",
        "            c=labels, cmap='tab20',\n",
        "            s=100*(1-uncertainties),\n",
        "            alpha=0.7,\n",
        "            edgecolor='k',\n",
        "            linewidths=0.5\n",
        "        )\n",
        "\n",
        "        texts = []\n",
        "        for cluster in np.unique(labels):\n",
        "            mask = labels == cluster\n",
        "            center = np.median(tsne_vectors[mask], axis=0)\n",
        "            distances = np.linalg.norm(tsne_vectors[mask] - center, axis=1)\n",
        "            closest_idx = np.argsort(distances)[:5]\n",
        "\n",
        "            for idx in closest_idx:\n",
        "                texts.append(plt.text(\n",
        "                    tsne_vectors[mask][idx,0], tsne_vectors[mask][idx,1],\n",
        "                    words[np.where(mask)[0][idx]],\n",
        "                    fontsize=9,\n",
        "                    bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1)\n",
        "                ))\n",
        "\n",
        "        adjust_text(texts)\n",
        "        plt.legend(\n",
        "            handles=scatter.legend_elements()[0],\n",
        "            labels=[f\"Кластер {i}\" for i in np.unique(labels)],\n",
        "            title=\"Кластеры\",\n",
        "            bbox_to_anchor=(1,1),\n",
        "            loc='upper left'\n",
        "        )\n",
        "        plt.title(\n",
        "            f\"Семантические кластеры (k={self.optimal_k})\\n\"\n",
        "            f\"Размер точки отражает уверенность кластеризации\\n\"\n",
        "            f\"Всего точек: {len(tsne_vectors)}\",\n",
        "            pad=20\n",
        "        )\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def hierarchical_clustering(self, max_subclusters=10, min_subclusters=3,\n",
        "                           max_points=200, min_cluster_size=20):\n",
        "        cluster_sizes = np.bincount(self.gmm.predict(self.reduced_vectors))\n",
        "\n",
        "        for cluster_idx in range(self.optimal_k):\n",
        "            size = cluster_sizes[cluster_idx]\n",
        "            if size < min_cluster_size:\n",
        "                print(f\"Кластер {cluster_idx} слишком мал ({size} точек), пропускаем\")\n",
        "                continue\n",
        "\n",
        "            cluster_mask = self.gmm.predict(self.reduced_vectors) == cluster_idx\n",
        "            cluster_points = self.reduced_vectors[cluster_mask]\n",
        "\n",
        "            if np.isnan(cluster_points).any():\n",
        "                print(f\"Кластер {cluster_idx} содержит NaN значения, пропускаем\")\n",
        "                continue\n",
        "\n",
        "            self._process_single_cluster(\n",
        "                cluster_idx,\n",
        "                max_subclusters,\n",
        "                min_subclusters,\n",
        "                max_points\n",
        "            )\n",
        "\n",
        "    def _process_single_cluster(self, cluster_idx, max_subclusters, min_subclusters, max_points):\n",
        "        \"\"\"Обработка одного кластера верхнего уровня с улучшенным анализом\"\"\"\n",
        "        cluster_mask = self.gmm.predict(self.reduced_vectors) == cluster_idx\n",
        "        cluster_points = self.reduced_vectors[cluster_mask]\n",
        "        cluster_words = [self.vocab[i] for i in np.where(cluster_mask)[0]]\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Анализ кластера {cluster_idx} ({len(cluster_points)} слов)\")\n",
        "        print(f\"Примеры слов: {', '.join(np.random.choice(cluster_words, min(10, len(cluster_words))))}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "\n",
        "        # Определение оптимального числа подкластеров\n",
        "        optimal_sub_k, metrics = self._find_optimal_subclusters(\n",
        "            cluster_points, max_subclusters, min_subclusters\n",
        "        )\n",
        "\n",
        "        if optimal_sub_k < 2:\n",
        "            print(\"Не удалось найти значимые подкластеры\")\n",
        "            return\n",
        "\n",
        "        # Кластеризация и визуализация\n",
        "        gmm_sub, sub_labels, original_indices = self._perform_subclustering(\n",
        "        cluster_points, cluster_words, cluster_idx, optimal_sub_k, max_points\n",
        "        )\n",
        "\n",
        "        self._store_subcluster_results(cluster_idx, cluster_mask, sub_labels, gmm_sub, original_indices)\n",
        "\n",
        "        # Визуализация метрик\n",
        "        self._plot_subcluster_metrics(metrics, optimal_sub_k)\n",
        "\n",
        "        # Анализ семантической когерентности подкластеров\n",
        "        self._analyze_subclusters_semantics(cluster_words, sub_labels)\n",
        "\n",
        "    def _find_optimal_subclusters(self, points, max_k, min_k):\n",
        "        \"\"\"Улучшенный метод определения оптимального числа подкластеров\"\"\"\n",
        "        sub_range = range(min_k, max_k+1)\n",
        "        metrics = {\n",
        "            'k': [],\n",
        "            'silhouette': [],\n",
        "            'davies_bouldin': [],\n",
        "            'bic': [],\n",
        "            'combo': [],\n",
        "            'gap': []\n",
        "        }\n",
        "\n",
        "        best_k = min_k\n",
        "        best_combo = -np.inf\n",
        "        prev_sil = -1\n",
        "\n",
        "        for k in sub_range:\n",
        "            metrics['k'].append(k)\n",
        "            if len(points) < k*2:\n",
        "                print(f\"Слишком мало точек ({len(points)}) для k={k}\")\n",
        "                for m in ['silhouette', 'davies_bouldin', 'bic', 'combo', 'gap']:\n",
        "                    metrics[m].append(None)\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                gmm = GaussianMixture(\n",
        "                    n_components=k,\n",
        "                    covariance_type='diag',\n",
        "                    random_state=self.random_state,\n",
        "                    n_init=5,\n",
        "                    reg_covar=1e-1,\n",
        "                    tol=1e-4,\n",
        "                    max_iter=500\n",
        "                )\n",
        "                gmm.fit(points)\n",
        "                labels = gmm.predict(points)\n",
        "\n",
        "                unique_labels = np.unique(labels)\n",
        "                if len(unique_labels) < k:\n",
        "                    raise ValueError(f\"Обнаружены пустые кластеры для k={k}\")\n",
        "\n",
        "                sil_score = silhouette_score(points, labels) if k > 1 else 0\n",
        "                db_score = davies_bouldin_score(points, labels) if k > 1 else np.inf\n",
        "                bic = gmm.bic(points)\n",
        "                combo_score = 0.7*sil_score - 0.3*db_score\n",
        "\n",
        "                try:\n",
        "                    ref_disps = []\n",
        "                    for _ in range(3):\n",
        "                        # Генерация синтетических данных\n",
        "                        random_data = np.random.rand(*points.shape) * (points.max(0)-points.min(0)) + points.min(0)\n",
        "\n",
        "                        # Фильтрация NaN/Inf\n",
        "                        random_data = np.nan_to_num(random_data, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "                        # Вычисление логарифма с защитой\n",
        "                        gmm_ref = GaussianMixture(\n",
        "                            n_components=k,\n",
        "                            random_state=self.random_state,\n",
        "                            reg_covar=1e-1\n",
        "                        )\n",
        "                        gmm_ref.fit(random_data)\n",
        "                        log_prob = gmm_ref.score(random_data)\n",
        "                        ref_disps.append(log_prob if log_prob > -np.inf else 0)\n",
        "\n",
        "                    current_score = gmm.score(points)\n",
        "                    gap = np.mean(ref_disps) - (current_score if current_score > -np.inf else 0)\n",
        "                except Exception as e:\n",
        "                    print(f\"Ошибка при вычислении Gap Statistic: {str(e)}\")\n",
        "                    gap = 0\n",
        "\n",
        "                metrics['silhouette'].append(sil_score)\n",
        "                metrics['davies_bouldin'].append(db_score)\n",
        "                metrics['bic'].append(bic)\n",
        "                metrics['combo'].append(combo_score)\n",
        "                metrics['gap'].append(gap)\n",
        "\n",
        "                # Обновление лучшего k\n",
        "                if combo_score > best_combo and gap > 0:\n",
        "                    best_combo = combo_score\n",
        "                    best_k = k\n",
        "\n",
        "                prev_sil = sil_score\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка для k={k}: {str(e)}\")\n",
        "                for m in ['silhouette', 'davies_bouldin', 'bic', 'combo', 'gap']:\n",
        "                    metrics[m].append(None)\n",
        "\n",
        "        return best_k, metrics\n",
        "\n",
        "    def _perform_subclustering(self, points, words, cluster_idx, optimal_k, max_points=500):\n",
        "        \"\"\"Улучшенная визуализация подкластеров с подписями всех точек\"\"\"\n",
        "        gmm_sub = GaussianMixture(\n",
        "                n_components=optimal_k,\n",
        "                covariance_type='diag',\n",
        "                random_state=self.random_state,\n",
        "                n_init=5,\n",
        "                reg_covar=1e-1,\n",
        "                tol=1e-4,\n",
        "                max_iter=500\n",
        "            )\n",
        "        sub_labels = gmm_sub.fit_predict(points)\n",
        "\n",
        "        # Проверка на NaN в метках\n",
        "        if np.isnan(sub_labels).any():\n",
        "            raise ValueError(\"Обнаружены NaN в метках кластеров\")\n",
        "\n",
        "        # Добавляем расчет неопределенности\n",
        "        probs = gmm_sub.predict_proba(points)\n",
        "        uncertainties = 1 - np.max(probs, axis=1)\n",
        "\n",
        "        # Сохраняем оригинальные индексы до выборки\n",
        "        original_indices = np.arange(len(points))\n",
        "\n",
        "        # Ограничиваем количество точек для визуализации\n",
        "        if len(points) > max_points:\n",
        "            indices = np.random.choice(len(points), max_points, replace=False)\n",
        "            points = points[indices]\n",
        "            words = [words[i] for i in indices]\n",
        "            sub_labels = sub_labels[indices]\n",
        "            uncertainties = uncertainties[indices]\n",
        "            original_indices = original_indices[indices]\n",
        "\n",
        "        # t-SNE проекция\n",
        "        perplexity = min(50, len(points)-1)\n",
        "        tsne = TSNE(\n",
        "            n_components=2,\n",
        "            perplexity=perplexity,\n",
        "            random_state=self.random_state,\n",
        "            init='pca',\n",
        "            learning_rate='auto'\n",
        "        )\n",
        "        embedded = tsne.fit_transform(points)\n",
        "\n",
        "        # Создаем фигуру\n",
        "        plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # Размер точек зависит от уверенности кластеризации (1 - uncertainty)\n",
        "        sizes = 50 + 150 * (1 - uncertainties)\n",
        "\n",
        "        # Цвета для подкластеров\n",
        "        colors = plt.cm.tab20(np.linspace(0, 1, optimal_k))\n",
        "\n",
        "        # Рисуем точки с подписями\n",
        "        texts = []\n",
        "        for i in range(len(points)):\n",
        "            plt.scatter(\n",
        "                embedded[i, 0], embedded[i, 1],\n",
        "                color=colors[sub_labels[i]],\n",
        "                s=sizes[i],\n",
        "                alpha=0.7,\n",
        "                edgecolor='k',\n",
        "                linewidths=0.3\n",
        "            )\n",
        "            texts.append(plt.text(\n",
        "                embedded[i, 0], embedded[i, 1],\n",
        "                words[i],\n",
        "                fontsize=8,\n",
        "                color='black',\n",
        "                ha='center', va='center'\n",
        "            ))\n",
        "\n",
        "        # Настройка легенды\n",
        "        legend_elements = [\n",
        "            plt.Line2D([0], [0], marker='o', color='w',\n",
        "                      markerfacecolor=colors[i], markersize=10,\n",
        "                      label=f'Подкластер {i}')\n",
        "            for i in range(optimal_k)\n",
        "        ]\n",
        "\n",
        "        plt.legend(\n",
        "            handles=legend_elements,\n",
        "            title='Подкластеры',\n",
        "            bbox_to_anchor=(1.05, 1),\n",
        "            loc='upper left'\n",
        "        )\n",
        "\n",
        "        # Настройка заголовка\n",
        "        plt.title(\n",
        "            f\"Разбиение кластера {cluster_idx} на {optimal_k} подкластеров\\n\"\n",
        "            f\"Размер точки отражает уверенность кластеризации\\n\"\n",
        "            f\"Всего точек: {len(points)}\",\n",
        "            pad=20\n",
        "        )\n",
        "\n",
        "        # Автоматическая регулировка подписей\n",
        "        adjust_text(\n",
        "            texts,\n",
        "            arrowprops=dict(arrowstyle='-', color='gray', lw=0.5),\n",
        "            expand_points=(1.2, 1.2),\n",
        "            expand_text=(1.1, 1.1),\n",
        "            force_text=(0.5, 0.5),\n",
        "            force_points=(0.8, 0.8),\n",
        "            lim=1000\n",
        "        )\n",
        "\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return gmm_sub, sub_labels, original_indices\n",
        "\n",
        "    def _plot_subcluster_metrics(self, metrics, optimal_k):\n",
        "        \"\"\"Исправленная визуализация метрик подкластеров\"\"\"\n",
        "        plt.figure(figsize=(15,10))\n",
        "\n",
        "        # Фильтруем невалидные значения\n",
        "        valid = ~np.isnan(metrics['silhouette'])\n",
        "        k_values = np.array(metrics['k'])[valid]\n",
        "\n",
        "        plt.subplot(2,2,1)\n",
        "        plt.plot(k_values, np.array(metrics['silhouette'])[valid], 'o-')\n",
        "        plt.axvline(optimal_k, color='r', linestyle='--')\n",
        "        plt.title('Silhouette Score')\n",
        "\n",
        "        plt.subplot(2,2,2)\n",
        "        plt.plot(k_values, np.array(metrics['davies_bouldin'])[valid], 'o-', color='orange')\n",
        "        plt.axvline(optimal_k, color='r', linestyle='--')\n",
        "        plt.title('Davies-Bouldin Index')\n",
        "\n",
        "        plt.subplot(2,2,3)\n",
        "        plt.plot(k_values, np.array(metrics['combo'])[valid], 'o-', color='green')\n",
        "        plt.axvline(optimal_k, color='r', linestyle='--')\n",
        "        plt.title('Комбинированная метрика')\n",
        "\n",
        "        plt.subplot(2,2,4)\n",
        "        plt.plot(k_values, np.array(metrics['gap'])[valid], 'o-', color='purple')\n",
        "        plt.axvline(optimal_k, color='r', linestyle='--')\n",
        "        plt.title('Gap Statistic')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def _store_subcluster_results(self, cluster_idx, mask, sub_labels, gmm_sub, original_indices):\n",
        "        self.sub_level_gmms[cluster_idx] = gmm_sub\n",
        "        cluster_global_indices = np.where(mask)[0]\n",
        "\n",
        "        for sub_id in range(gmm_sub.n_components):\n",
        "            sub_mask = sub_labels == sub_id\n",
        "            global_indices = cluster_global_indices[original_indices[sub_mask]]\n",
        "            for idx in global_indices:\n",
        "                self.stationary_indices[idx].append((cluster_idx, sub_id))\n",
        "\n",
        "    def _analyze_subclusters_semantics(self, words, labels):\n",
        "        unique_labels = np.unique(labels)\n",
        "        print(\"\\nСемантический анализ подкластеров:\")\n",
        "\n",
        "        for sub_id in unique_labels:\n",
        "            cluster_words = [w for w, l in zip(words, labels) if l == sub_id]\n",
        "            print(f\"\\nПодкластер {sub_id} ({len(cluster_words)} слов):\")\n",
        "            print(\", \".join(cluster_words[:15]) + (\"...\" if len(cluster_words) > 15 else \"\"))\n",
        "\n",
        "    def get_cluster_words(self, top_n=10, sort_clusters=True):\n",
        "        \"\"\"Получение топ-слов с вероятностями и сортировкой по кластерам\"\"\"\n",
        "        cluster_words = defaultdict(list)\n",
        "        probs = self.gmm.predict_proba(self.reduced_vectors)\n",
        "\n",
        "        for word, label, prob in zip(self.vocab,\n",
        "                                   self.gmm.predict(self.reduced_vectors),\n",
        "                                   probs):\n",
        "            cluster_words[label].append((word, np.max(prob)))\n",
        "\n",
        "        result = {}\n",
        "        for k in cluster_words:\n",
        "            sorted_words = sorted(cluster_words[k], key=lambda x: -x[1])\n",
        "            result[k] = [w[0] for w in sorted_words[:top_n]]\n",
        "\n",
        "        return dict(sorted(result.items())) if sort_clusters else result\n",
        "\n",
        "    def print_cluster_words(self, top_n=15):\n",
        "        \"\"\"Улучшенный вывод с информацией о размерах и вероятностях\"\"\"\n",
        "        cluster_words = self.get_cluster_words(top_n=top_n)\n",
        "        labels = self.gmm.predict(self.reduced_vectors)\n",
        "        cluster_sizes = np.bincount(labels)\n",
        "        probs = self.gmm.predict_proba(self.reduced_vectors)\n",
        "\n",
        "        print(\"\\nТоп слов по кластерам (размер | средняя вероятность):\")\n",
        "        for cluster in sorted(cluster_words.keys()):\n",
        "            cluster_probs = probs[labels == cluster]\n",
        "            avg_prob = np.mean(np.max(cluster_probs, axis=1)) if len(cluster_probs) > 0 else 0\n",
        "\n",
        "            print(f\"\\nКластер {cluster} [размер: {cluster_sizes[cluster]} | prob: {avg_prob:.2f}]:\")\n",
        "            print(\", \".join(cluster_words[cluster][:top_n]))\n",
        "\n",
        "    def analyze_cluster_quality(self, sample_size=1000):\n",
        "        labels = self.gmm.predict(self.reduced_vectors)\n",
        "        cluster_coherence = {}\n",
        "        cluster_diversity = {}\n",
        "        cluster_sizes = np.bincount(labels)\n",
        "\n",
        "        for cluster in np.unique(labels):\n",
        "            mask = labels == cluster\n",
        "            cluster_vectors = self.vectors[mask]\n",
        "            cluster_words = [self.vocab[i] for i in np.where(mask)[0]]\n",
        "\n",
        "            similarities = cosine_similarity(cluster_vectors)\n",
        "            np.fill_diagonal(similarities, 0)\n",
        "            cluster_coherence[cluster] = np.mean(similarities)\n",
        "\n",
        "            diversities = []\n",
        "            for word in cluster_words[:sample_size]:\n",
        "                word_vec = self.vectors[self.word_to_idx[word]]\n",
        "                sims = cosine_similarity([word_vec], self.vectors)[0]\n",
        "                top_indices = np.argsort(-sims)[1:11]\n",
        "                top_words = [self.vocab[i] for i in top_indices]\n",
        "                counts = [cluster_words.count(w) for w in top_words]\n",
        "                probs = np.array(counts) / len(cluster_words)\n",
        "                probs = probs[probs > 0]\n",
        "                diversities.append(entropy(probs))\n",
        "\n",
        "            cluster_diversity[cluster] = np.mean(diversities) if diversities else 0\n",
        "\n",
        "        self._plot_cluster_quality(cluster_coherence, cluster_diversity, cluster_sizes)\n",
        "        return {\n",
        "            'coherence': cluster_coherence,\n",
        "            'diversity': cluster_diversity,\n",
        "            'cluster_sizes': cluster_sizes\n",
        "        }\n",
        "\n",
        "    def _plot_cluster_quality(self, coherence, diversity, sizes):\n",
        "        \"\"\"Обновленная визуализация с размерами кластеров\"\"\"\n",
        "        plt.figure(figsize=(18,6))\n",
        "        clusters = sorted(coherence.keys())\n",
        "\n",
        "        # График когерентности с цветовой индикацией размеров\n",
        "        plt.subplot(131)\n",
        "        max_size = max(sizes)\n",
        "        colors = [plt.cm.viridis(sizes[c]/max_size) for c in clusters]  # Исправлено здесь\n",
        "\n",
        "        bars = plt.bar(clusters,\n",
        "                      [coherence[c] for c in clusters],\n",
        "                      color=colors,\n",
        "                      alpha=0.7)\n",
        "\n",
        "        # Аннотации с размерами\n",
        "        for idx, bar in enumerate(bars):\n",
        "            plt.text(bar.get_x() + bar.get_width()/2,\n",
        "                    bar.get_height(),\n",
        "                    f'{sizes[idx]}',\n",
        "                    ha='center',\n",
        "                    va='bottom',\n",
        "                    fontsize=8)\n",
        "\n",
        "        plt.title('Когерентность и размер кластеров\\n(цвет отражает относительный размер)')\n",
        "        plt.xlabel('Кластер')\n",
        "        plt.ylabel('Когерентность')\n",
        "\n",
        "        plt.subplot(132)\n",
        "        plt.bar(clusters, [diversity[c] for c in clusters], color='orange', alpha=0.7)\n",
        "        plt.title('Диверсификация кластеров')\n",
        "\n",
        "        plt.subplot(133)\n",
        "        plt.bar(clusters, [sizes[c] for c in clusters], color='green', alpha=0.7)\n",
        "        plt.yscale('log')\n",
        "        plt.title('Размеры кластеров (лог. шкала)')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def analyze_covariance_diff(self, cluster_idx1, cluster_idx2, top_n=5, top_k=10):\n",
        "        gmm1 = self.sub_level_gmms[cluster_idx1]\n",
        "        gmm2 = self.sub_level_gmms[cluster_idx2]\n",
        "\n",
        "        mask1 = self.gmm.predict(self.reduced_vectors) == cluster_idx1\n",
        "        mask2 = self.gmm.predict(self.reduced_vectors) == cluster_idx2\n",
        "        points1 = self.reduced_vectors[mask1]\n",
        "        points2 = self.reduced_vectors[mask2]\n",
        "        words1 = [self.vocab[i] for i in np.where(mask1)[0]]\n",
        "        words2 = [self.vocab[i] for i in np.where(mask2)[0]]\n",
        "\n",
        "        kl_divs = []\n",
        "        for dim in range(self.reduced_vectors.shape[1]):\n",
        "            hist1, _ = np.histogram(points1[:, dim], bins=20, density=True)\n",
        "            hist2, _ = np.histogram(points2[:, dim], bins=20, density=True)\n",
        "\n",
        "            hist1 = (hist1 + 1e-10) / (hist1.sum() + 1e-10)\n",
        "            hist2 = (hist2 + 1e-10) / (hist2.sum() + 1e-10)\n",
        "\n",
        "            kl_div = entropy(hist1, hist2) + entropy(hist2, hist1)\n",
        "            kl_divs.append(kl_div)\n",
        "\n",
        "        top_dims = np.argsort(-np.array(kl_divs))[:top_n]\n",
        "\n",
        "        results = []\n",
        "        for dim in top_dims:\n",
        "            comp1_words = [self.vocab[i] for i in np.where(mask1)[0]]\n",
        "            comp2_words = [self.vocab[i] for i in np.where(mask2)[0]]\n",
        "\n",
        "            top1 = self._get_top_words_by_dim(comp1_words, points1, dim, top_k)\n",
        "            top2 = self._get_top_words_by_dim(comp2_words, points2, dim, top_k)\n",
        "\n",
        "            results.append({\n",
        "                'dimension': dim,\n",
        "                'kl_divergence': kl_divs[dim],\n",
        "                'cluster1_top': top1,\n",
        "                'cluster2_top': top2\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _get_top_words_by_dim(self, words, vectors, dim, n_words, ascending=False):\n",
        "        indices = np.argsort(vectors[:, dim])\n",
        "        if not ascending: indices = indices[::-1]\n",
        "        return [words[i] for i in indices[:n_words]]\n",
        "\n",
        "    def print_covariance_analysis(self, results):\n",
        "        print(\"\\nАнализ различий между кластерами:\")\n",
        "        for res in results:\n",
        "            print(f\"\\nИзмерение {res['dimension']} (KL: {res['kl_divergence']:.3f})\")\n",
        "            print(f\"Топ кластер 1: {', '.join(res['cluster1_top'][:5])}\")\n",
        "            print(f\"Топ кластер 2: {', '.join(res['cluster2_top'][:5])}\")\n",
        "\n",
        "    def compare_clusters_visually(self, cluster_indices, max_points=500, label_top_n=20):\n",
        "        combined_mask = np.zeros(len(self.reduced_vectors), dtype=bool)\n",
        "        for idx in cluster_indices:\n",
        "            combined_mask |= (self.gmm.predict(self.reduced_vectors) == idx)\n",
        "\n",
        "        points = self.reduced_vectors[combined_mask]\n",
        "        words = [self.vocab[i] for i in np.where(combined_mask)[0]]\n",
        "\n",
        "        tsne = TSNE(n_components=2, random_state=42)\n",
        "        embedded = tsne.fit_transform(points)\n",
        "\n",
        "        plt.figure(figsize=(20,16))\n",
        "        texts = []\n",
        "        for i in range(len(points)):\n",
        "            plt.scatter(embedded[i,0], embedded[i,1], s=50, alpha=0.7)\n",
        "            texts.append(plt.text(embedded[i,0], embedded[i,1], words[i], fontsize=8))\n",
        "\n",
        "        adjust_text(texts)\n",
        "        plt.title(f\"Сравнение кластеров {cluster_indices}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "S0UM1mPdWoU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    analyzer = BertSemanticAnalyzer()\n",
        "\n",
        "    sentences = analyzer.extract_sentences(\"annot.opcorpora.xml\")\n",
        "    nouns = analyzer.extract_frequent_nouns(sentences, top_n=10000)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    vectors = analyzer.get_contextual_embeddings(\n",
        "        sentences,\n",
        "        nouns,\n",
        "        batch_size=64,\n",
        "        max_contexts_per_noun=15,\n",
        "        sample_size=None\n",
        "    )\n",
        "\n",
        "    # Анализ\n",
        "    analyzer.reduce_dimensions(n_components=300, variance_threshold=0.0025)\n",
        "    analyzer.find_optimal_clusters(max_k=150, min_k=50, step=10)\n",
        "\n",
        "    # Визуализация и отчет\n",
        "    analyzer.visualize_clusters(max_points=2000)\n",
        "    analyzer.print_cluster_words(top_n=15)\n",
        "    quality_report = analyzer.analyze_cluster_quality()\n",
        "\n",
        "    # Иерархическая кластеризация с новыми параметрами\n",
        "    analyzer.hierarchical_clustering(\n",
        "        max_subclusters=5,\n",
        "        min_subclusters=3,\n",
        "        max_points=200,\n",
        "        min_cluster_size=60\n",
        "    )\n",
        "\n",
        "    # Дополнительный анализ\n",
        "    if len(analyzer.sub_level_gmms) >= 2:\n",
        "        valid_clusters = list(analyzer.sub_level_gmms.keys())\n",
        "\n",
        "        if quality_report and 'coherence' in quality_report:\n",
        "            cluster_coherence = quality_report['coherence']\n",
        "            filtered_clusters = [(k, cluster_coherence[k]) for k in valid_clusters if k in cluster_coherence]\n",
        "            sorted_clusters = sorted(filtered_clusters, key=lambda x: x[1])\n",
        "\n",
        "            if len(sorted_clusters) >= 2:\n",
        "                cluster1 = sorted_clusters[0][0]\n",
        "                cluster2 = sorted_clusters[-1][0]\n",
        "            else:\n",
        "                cluster1, cluster2 = valid_clusters[:2]\n",
        "        else:\n",
        "            cluster1, cluster2 = valid_clusters[:2]\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"Сравниваем кластеры {cluster1} и {cluster2}\")\n",
        "        if quality_report and 'coherence' in quality_report:\n",
        "            print(f\"Когерентность: {cluster_coherence.get(cluster1, 0):.3f} vs {cluster_coherence.get(cluster2, 0):.3f}\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        cov_results = analyzer.analyze_covariance_diff(cluster1, cluster2)\n",
        "        analyzer.print_covariance_analysis(cov_results)\n",
        "        analyzer.compare_clusters_visually([cluster1, cluster2])\n",
        "    else:\n",
        "        print(\"\\nНедостаточно кластеров для сравнения\")"
      ],
      "metadata": {
        "id": "oE7PQItd5_5x"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}